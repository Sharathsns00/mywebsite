# -*- coding: utf-8 -*-
"""Copy of app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FujTNFQM-St9bKCvBrkh3T2npGkNYrEn
"""

# ==============================================================================
# STEP 1: INSTALLATION
# ==============================================================================
print("üîß Installing required packages...")

!pip install -q streamlit transformers[torch] accelerate PyPDF2 sentence-transformers faiss-cpu pyngrok bitsandbytes torch
!npm install -g localtunnel
print("üéâ Installation complete!")

# app.py
# ================================================================
# Granite PDF Chatbot with IBM Granite 3.0 2B Instruct
# ================================================================

import streamlit as st
import PyPDF2
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import re
import gc
import warnings
warnings.filterwarnings("ignore")


# ---------------------------------------------------------
# STREAMLIT PAGE CONFIG (must run before other st.* calls)
# ---------------------------------------------------------
st.set_page_config(
    page_title="ü§ñ Granite PDF Chatbot",
    page_icon="üìö",
    layout="wide",
    initial_sidebar_state="expanded"
)


# ---------------------------------------------------------
# GRANITE PDF CHATBOT CLASS
# ---------------------------------------------------------
class GranitePDFChatbot:
    def _init_(self):
        # Model names
        self.model_name = "ibm-granite/granite-3.0-2b-instruct"
        self.embedding_model_name = "sentence-transformers/all-MiniLM-L6-v2"

        # Model handles (set when initialize_models runs)
        self.tokenizer = None
        self.model = None
        self.embedding_model = None

        # Document storage / search
        self.document_chunks = []
        self.embeddings = None
        self.faiss_index = None

        # Flags (explicitly set to avoid attribute errors)
        self.models_loaded = False
        self.document_processed = False

    def check_gpu_memory(self):
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            gpu_memory_used = torch.cuda.memory_allocated(0) / (1024**3)
            gpu_memory_free = gpu_memory - gpu_memory_used
            return gpu_memory, gpu_memory_used, gpu_memory_free
        return None, None, None

    def initialize_models(self):
        if getattr(self, "models_loaded", False):
            return True

        try:
            with st.spinner("üîÑ Loading tokenizer..."):
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name,
                    trust_remote_code=True
                )
                if getattr(self.tokenizer, "pad_token", None) is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                st.success("‚úÖ Tokenizer loaded!")

            with st.spinner("üîÑ Loading Granite model..."):
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16
                )
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    quantization_config=bnb_config,
                    device_map="auto",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                st.success("‚úÖ Granite model loaded!")

            with st.spinner("üîÑ Loading embedding model..."):
                self.embedding_model = SentenceTransformer(
                    self.embedding_model_name,
                    device='cuda' if torch.cuda.is_available() else 'cpu'
                )
                st.success("‚úÖ Embedding model loaded!")

            self.models_loaded = True
            st.success("üéâ All models loaded! Upload a PDF now.")
            return True

        except Exception as e:
            st.error(f"‚ùå Error loading models: {str(e)}")
            self.models_loaded = False
            return False

    def extract_pdf_text(self, pdf_file) -> str:
        try:
            pdf_file.seek(0)
            pdf_reader = PyPDF2.PdfReader(pdf_file)

            text_content = ""
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    page_text = page.extract_text()
                except Exception:
                    page_text = None
                if page_text:
                    text_content += f"--- Page {page_num+1} ---\n" + page_text + "\n"
            return text_content.strip()
        except Exception as e:
            st.error(f"‚ùå Error extracting PDF: {e}")
            return ""

    def create_text_chunks(self, text: str, chunk_size: int = 800, overlap: int = 100):
        if not text:
            return []
        text = re.sub(r'\s+', ' ', text).strip()
        sentences = re.split(r'(?<=[.!?])\s+', text)

        chunks = []
        current_chunk = ""
        for sentence in sentences:
            if not sentence:
                continue
            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                words = current_chunk.split()
                overlap_words = words[-overlap:] if len(words) > overlap else words
                current_chunk = " ".join(overlap_words) + " " + sentence
            else:
                current_chunk += (" " + sentence) if current_chunk else sentence
        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        # filter very short chunks
        return [c for c in chunks if len(c) > 50]

    def build_vector_database(self, text: str):
        if not getattr(self, "models_loaded", False):
            st.error("‚ùå Models are not loaded. Please initialize models first.")
            return False

        self.document_chunks = self.create_text_chunks(text)
        if not self.document_chunks:
            st.error("‚ùå No chunks created")
            return False

        # Generate embeddings safely (different versions return different types)
        try:
            all_embeddings = self.embedding_model.encode(self.document_chunks, convert_to_numpy=True)
        except TypeError:
            # fallback for older/newer APIs
            try:
                all_embeddings = np.array([self.embedding_model.encode(x) for x in self.document_chunks])
            except Exception as e:
                st.error(f"‚ùå Embedding generation failed: {e}")
                return False

        self.embeddings = np.array(all_embeddings)
        if self.embeddings.ndim != 2:
            st.error("‚ùå Unexpected embedding shape.")
            return False

        dimension = self.embeddings.shape[1]
        self.faiss_index = faiss.IndexFlatL2(dimension)
        self.faiss_index.add(self.embeddings.astype('float32'))
        self.document_processed = True
        return True

    def search_relevant_chunks(self, query: str, top_k: int = 3):
        if not self.faiss_index or not self.document_chunks:
            return []
        query_embedding = self.embedding_model.encode([query])
        q = np.array(query_embedding).astype('float32')
        k = min(top_k, len(self.document_chunks))
        scores, indices = self.faiss_index.search(q, k)
        return [self.document_chunks[i] for i in indices[0] if i < len(self.document_chunks)]

    def generate_answer(self, question: str, context_chunks):
        if not getattr(self, "models_loaded", False):
            return "‚ùå Models not loaded. Please initialize models first."

        context = "\n\n".join(context_chunks) if context_chunks else "No context"
        system_message = "You are an assistant. Answer using only the provided context."
        user_message = f"Context:\n{context}\n\nQuestion: {question}\nAnswer:"

        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ]

        # apply_chat_template may not exist; fall back to concatenated prompt
        try:
            prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        except Exception:
            prompt = system_message + "\n\n" + user_message

        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1800).to(self.model.device)
            outputs = self.model.generate(**inputs, max_new_tokens=400, temperature=0.7, top_p=0.9)
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        except Exception as e:
            return f"‚ùå Error during generation: {e}"

        if "Answer:" in full_response:
            return full_response.split("Answer:")[-1].strip()
        return full_response


# ---------------------------------------------------------
# STREAMLIT APP (defensive session handling)
# ---------------------------------------------------------
def get_chatbot():
    """
    Ensure st.session_state['chatbot'] is a valid GranitePDFChatbot instance
    and has the expected attributes. Recreate if stale or invalid.
    """
    obj = st.session_state.get("chatbot", None)
    if obj is None:
        st.session_state["chatbot"] = GranitePDFChatbot()
        return st.session_state["chatbot"]

    # If it's already the right class and has 'models_loaded', return it
    if isinstance(obj, GranitePDFChatbot) and hasattr(obj, "models_loaded"):
        return obj

    # Otherwise recreate and replace
    st.warning("‚ö† Found stale/invalid chatbot in session state ‚Äî reinitializing.")
    st.session_state["chatbot"] = GranitePDFChatbot()
    return st.session_state["chatbot"]


def main():
    st.title("ü§ñ Granite PDF Chatbot")
    st.markdown("Upload a PDF and ask questions!")

    # Always get a validated chatbot instance
    chatbot = get_chatbot()

    # Show GPU memory if available
    total_mem, used_mem, free_mem = chatbot.check_gpu_memory()
    if total_mem:
        st.caption(f"GPU memory: {free_mem:.1f} GB free / {total_mem:.1f} GB total")

    # Initialize models button (safe attribute access via getattr)
    if not getattr(chatbot, "models_loaded", False):
        if st.button("üöÄ Initialize Models"):
            chatbot.initialize_models()
    else:
        st.success("‚úÖ Models ready")

    uploaded_file = st.file_uploader("üìÑ Upload PDF", type="pdf")
    if uploaded_file and getattr(chatbot, "models_loaded", False):
        if st.button("üîÑ Process Document"):
            text = chatbot.extract_pdf_text(uploaded_file)
            if not text:
                st.error("‚ùå No extractable text found in the PDF.")
            else:
                built = chatbot.build_vector_database(text)
                if built:
                    st.success("‚úÖ Document processed! You can now ask questions.")
                else:
                    st.error("‚ùå Failed to build vector DB from document.")

    if getattr(chatbot, "document_processed", False):
        question = st.text_input("üí¨ Ask a question")
        if question:
            with st.spinner("Searching the document and generating answer..."):
                chunks = chatbot.search_relevant_chunks(question, top_k=3)
                answer = chatbot.generate_answer(question, chunks)
            st.markdown(f"### ü§ñ Answer:\n{answer}")


if __name__ == "__main__":
    main()

# üöÄ Premium IBM Granite 3.0 2b Chatbot - Hackathon Edition
# Ultra-Advanced Features with Stunning Colorful UI

# Step 1: Install all required packages
!pip install transformers torch accelerate
!pip install PyPDF2 python-docx
!pip install gradio plotly
!pip install sentence-transformers
!pip install langchain langchain-community
!pip install chromadb datasets
!pip install wordcloud matplotlib seaborn
!pip install textstat nltk pandas numpy
!pip install Pillow requests beautifulsoup4

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import PyPDF2
import io
import gradio as gr
import json
import re
from typing import List, Dict, Any
import random
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
import textstat
import base64
from collections import Counter
import time
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import hashlib

# üåà Premium GraniteBot with Advanced Features
class PremiumGraniteBot:
    def _init_(self):
        self.model_name = "ibm-granite/granite-3.0-2b-instruct"
        print("üöÄ Loading Premium IBM Granite 3.0 2b AI...")

        # Initialize model components
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )

        self.generator = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )

        # Advanced storage and analytics
        self.pdf_content = ""
        self.qa_pairs = []
        self.quiz_questions = []
        self.chat_history = []
        self.user_performance = {"correct": 0, "total": 0, "streak": 0}
        self.session_stats = {
            "start_time": datetime.now(),
            "documents_processed": 0,
            "questions_generated": 0,
            "chats_completed": 0,
            "avg_response_time": 0,
            "user_satisfaction": []
        }

        # AI personalities for different modes
        self.personalities = {
            "friendly": "You are a friendly and encouraging AI tutor who makes learning fun!",
            "professional": "You are a professional and precise AI assistant focused on accuracy.",
            "creative": "You are a creative and innovative AI that thinks outside the box!",
            "scholar": "You are a scholarly AI with deep knowledge and analytical thinking.",
            "mentor": "You are a wise mentor who guides users to discover answers themselves."
        }

        print("‚úÖ Premium AI System Ready!")

    def extract_text_from_pdf(self, pdf_file):
        """üîç Advanced PDF extraction with detailed analytics"""
        try:
            if isinstance(pdf_file, str):
                with open(pdf_file, 'rb') as f:
                    pdf_data = f.read()
            else:
                pdf_data = pdf_file

            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_data))
            text = ""
            page_texts = []

            for page_num in range(len(pdf_reader.pages)):
                page = pdf_reader.pages[page_num]
                page_text = page.extract_text()
                text += page_text + "\n"
                page_texts.append(page_text)

            # Advanced analytics
            words = text.split()
            sentences = re.split(r'[.!?]+', text)

            self.document_analytics = {
                "total_pages": len(pdf_reader.pages),
                "total_characters": len(text),
                "total_words": len(words),
                "total_sentences": len([s for s in sentences if s.strip()]),
                "avg_words_per_page": len(words) / len(pdf_reader.pages) if pdf_reader.pages else 0,
                "reading_time_minutes": len(words) / 200,  # Average reading speed
                "complexity_score": textstat.flesch_reading_ease(text),
                "grade_level": textstat.flesch_kincaid_grade(text),
                "page_texts": page_texts
            }

            self.session_stats["documents_processed"] += 1
            return text.strip()

        except Exception as e:
            return f"‚ùå Error extracting PDF: {str(e)}"

    def generate_smart_questions(self, text: str, num_questions: int = 5, difficulty: str = "mixed", question_type: str = "comprehensive"):
        """üß† AI-powered question generation with advanced algorithms"""
        chunks = self.smart_chunk_text(text, 800)
        all_qa_pairs = []

        question_templates = {
            "factual": "Generate factual questions that test specific information, dates, names, and concrete details.",
            "analytical": "Generate analytical questions that require critical thinking, comparison, and evaluation.",
            "conceptual": "Generate conceptual questions that test understanding of main ideas, themes, and principles.",
            "application": "Generate application questions that test how knowledge can be used in real scenarios.",
            "comprehensive": "Generate a diverse mix of factual, analytical, conceptual, and application questions."
        }

        difficulty_modifiers = {
            "easy": "Focus on basic recall and simple understanding. Use clear, straightforward language.",
            "medium": "Include some analysis and interpretation. Mix different cognitive levels.",
            "hard": "Emphasize critical thinking, synthesis, and complex analysis.",
            "mixed": "Vary difficulty levels from easy recall to complex analysis."
        }

        for i, chunk in enumerate(chunks[:min(3, len(chunks))]):
            prompt = f"""You are an expert educational content creator. Based on the following text, create {num_questions} high-quality questions.

{question_templates[question_type]}
{difficulty_modifiers[difficulty]}

Text: {chunk}

Format each question-answer pair exactly as:
Q: [Your question here]
A: [Comprehensive answer here]
Difficulty: [Easy/Medium/Hard]
Type: [Factual/Analytical/Conceptual/Application]
Points: [1-5 based on difficulty]

Create engaging, thought-provoking questions:"""

            try:
                response = self.generator(
                    prompt,
                    max_new_tokens=600,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )

                generated_text = response[0]['generated_text'][len(prompt):]
                qa_pairs = self.parse_enhanced_qa_response(generated_text)
                all_qa_pairs.extend(qa_pairs)

            except Exception as e:
                print(f"‚ùå Error generating questions for chunk {i+1}: {e}")

        # Remove duplicates and ensure quality
        unique_questions = []
        seen_questions = set()

        for qa in all_qa_pairs:
            q_hash = hashlib.md5(qa.get('question', '').lower().encode()).hexdigest()
            if q_hash not in seen_questions and len(qa.get('question', '')) > 10:
                seen_questions.add(q_hash)
                unique_questions.append(qa)

        self.session_stats["questions_generated"] += len(unique_questions)
        return unique_questions[:num_questions]

    def smart_chunk_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        """üìù Intelligent text chunking that preserves context"""
        sentences = re.split(r'(?<=[.!?])\s+', text)
        chunks = []
        current_chunk = ""

        for sentence in sentences:
            if len(current_chunk) + len(sentence) < chunk_size:
                current_chunk += sentence + " "
            else:
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + " "

        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return chunks

    def parse_enhanced_qa_response(self, response: str) -> List[Dict]:
        """üîç Advanced parsing with comprehensive metadata"""
        qa_pairs = []
        lines = [line.strip() for line in response.split('\n') if line.strip()]
        current_qa = {}

        for line in lines:
            if line.startswith('Q:'):
                if current_qa.get('question') and current_qa.get('answer'):
                    qa_pairs.append(current_qa)
                current_qa = {
                    "question": line[2:].strip(),
                    "answer": "",
                    "difficulty": "Medium",
                    "type": "Mixed",
                    "points": 3,
                    "timestamp": datetime.now().isoformat()
                }
            elif line.startswith('A:'):
                current_qa["answer"] = line[2:].strip()
            elif line.startswith('Difficulty:'):
                current_qa["difficulty"] = line[11:].strip()
            elif line.startswith('Type:'):
                current_qa["type"] = line[5:].strip()
            elif line.startswith('Points:'):
                try:
                    current_qa["points"] = int(line[7:].strip())
                except:
                    current_qa["points"] = 3
            elif current_qa.get('answer') and line and not line.startswith(('Q:', 'A:', 'Difficulty:', 'Type:', 'Points:')):
                current_qa["answer"] += " " + line

        # Add the last Q&A pair
        if current_qa.get('question') and current_qa.get('answer'):
            qa_pairs.append(current_qa)

        return qa_pairs

    def generate_interactive_quiz(self, num_questions: int = 5, difficulty: str = "mixed", quiz_type: str = "multiple_choice"):
        """üéØ Generate interactive quizzes with multiple formats"""
        if not self.pdf_content:
            return []

        chunks = self.smart_chunk_text(self.pdf_content, 600)
        quiz_questions = []

        quiz_formats = {
            "multiple_choice": "Create multiple choice questions with 4 options (A, B, C, D).",
            "true_false": "Create true/false questions with explanations.",
            "fill_blank": "Create fill-in-the-blank questions.",
            "short_answer": "Create short answer questions requiring 2-3 sentence responses."
        }

        for i, chunk in enumerate(chunks[:num_questions]):
            prompt = f"""Based on this text, {quiz_formats[quiz_type]}
            Make the question challenging but fair, testing important concepts.

            Text: {chunk}

            Format:
            Question: [Your question here]
            A) [Option A]
            B) [Option B]
            C) [Option C]
            D) [Option D]
            Correct Answer: [A/B/C/D]
            Explanation: [Detailed explanation why this is correct]
            Difficulty: [Easy/Medium/Hard]
            Points: [1-5 based on difficulty]
            Learning Objective: [What this tests]"""

            try:
                response = self.generator(
                    prompt,
                    max_new_tokens=400,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

                generated_text = response[0]['generated_text'][len(prompt):]
                quiz_question = self.parse_quiz_response(generated_text)
                if quiz_question:
                    quiz_question.update({
                        "id": i + 1,
                        "type": quiz_type,
                        "timestamp": datetime.now().isoformat()
                    })
                    quiz_questions.append(quiz_question)

            except Exception as e:
                print(f"‚ùå Error generating quiz question {i+1}: {e}")

        return quiz_questions

    def parse_quiz_response(self, response: str) -> Dict:
        """üéØ Advanced quiz parsing with comprehensive metadata"""
        lines = [line.strip() for line in response.split('\n') if line.strip()]
        quiz_data = {
            "points": 3,
            "difficulty": "Medium",
            "learning_objective": "Comprehension"
        }

        for line in lines:
            if line.startswith('Question:'):
                quiz_data['question'] = line[9:].strip()
            elif line.startswith('A)'):
                quiz_data['option_a'] = line[2:].strip()
            elif line.startswith('B)'):
                quiz_data['option_b'] = line[2:].strip()
            elif line.startswith('C)'):
                quiz_data['option_c'] = line[2:].strip()
            elif line.startswith('D)'):
                quiz_data['option_d'] = line[2:].strip()
            elif line.startswith('Correct Answer:'):
                quiz_data['correct_answer'] = line[15:].strip()
            elif line.startswith('Explanation:'):
                quiz_data['explanation'] = line[12:].strip()
            elif line.startswith('Difficulty:'):
                quiz_data['difficulty'] = line[11:].strip()
            elif line.startswith('Points:'):
                try:
                    quiz_data['points'] = int(line[7:].strip())
                except:
                    quiz_data['points'] = 3
            elif line.startswith('Learning Objective:'):
                quiz_data['learning_objective'] = line[19:].strip()

        return quiz_data if len(quiz_data) >= 8 else None

    def enhanced_chat_response(self, message: str, personality: str = "friendly", use_context: bool = True) -> str:
        """üí¨ Advanced chat with personality and context awareness"""
        if not message.strip():
            return "üí≠ I'm here to help! What would you like to know?"

        context = ""
        if use_context and self.pdf_content:
            context = f"Document context: {self.pdf_content[:1000]}...\n\n"

        personality_prompt = self.personalities.get(personality, self.personalities["friendly"])

        prompt = f"""{context}{personality_prompt}

Previous conversation context: {self.get_recent_chat_context()}

User: {message}
Assistant: """

        try:
            start_time = time.time()
            response = self.generator(
                prompt,
                max_new_tokens=300,
                temperature=0.8,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
            response_time = time.time() - start_time

            generated_text = response[0]['generated_text'][len(prompt):].strip()

            # Update analytics
            self.chat_history.append({
                "user": message,
                "assistant": generated_text,
                "timestamp": datetime.now().isoformat(),
                "personality": personality,
                "response_time": response_time
            })

            self.session_stats["chats_completed"] += 1
            self.update_avg_response_time(response_time)

            return generated_text

        except Exception as e:
            return f"üö® I encountered an error: {str(e)}. Please try again!"

    def get_recent_chat_context(self, num_messages: int = 3) -> str:
        """üìö Get recent chat context for better responses"""
        if not self.chat_history:
            return ""

        recent_chats = self.chat_history[-num_messages:]
        context = ""
        for chat in recent_chats:
            context += f"User: {chat['user']}\nAssistant: {chat['assistant']}\n"

        return context

    def update_avg_response_time(self, new_time: float):
        """‚è± Update average response time"""
        current_avg = self.session_stats["avg_response_time"]
        total_chats = self.session_stats["chats_completed"]

        if total_chats == 1:
            self.session_stats["avg_response_time"] = new_time
        else:
            self.session_stats["avg_response_time"] = (current_avg * (total_chats - 1) + new_time) / total_chats

    def generate_document_insights(self):
        """üìä Generate comprehensive document analytics and visualizations"""
        if not self.pdf_content:
            return None, None, None, ""

        # Word frequency analysis
        words = re.findall(r'\b[a-zA-Z]+\b', self.pdf_content.lower())
        word_freq = Counter([w for w in words if len(w) > 3])
        top_words = word_freq.most_common(15)

        # Create enhanced word frequency chart
        df_words = pd.DataFrame(top_words, columns=['Word', 'Frequency'])
        fig_words = px.bar(
            df_words,
            x='Frequency',
            y='Word',
            orientation='h',
            title="üìä Top 15 Most Frequent Words",
            color='Frequency',
            color_continuous_scale='plasma',
            text='Frequency'
        )
        fig_words.update_layout(
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            font=dict(color='white', size=12),
            title=dict(x=0.5, font=dict(size=18)),
            height=500
        )
        fig_words.update_traces(texttemplate='%{text}', textposition='outside')

        # Document structure analysis
        stats = self.document_analytics
        fig_structure = go.Figure()

        # Add multiple traces for different metrics
        metrics = ['Pages', 'Words', 'Sentences']
        values = [stats['total_pages'], stats['total_words']//100, stats['total_sentences']//10]
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']

        fig_structure.add_trace(go.Bar(
            x=metrics,
            y=values,
            marker_color=colors,
            text=values,
            textposition='auto',
        ))

        fig_structure.update_layout(
            title="üìà Document Structure Analysis",
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            font=dict(color='white', size=12),
            title=dict(x=0.5, font=dict(size=18)),
            showlegend=False
        )

        # Reading complexity analysis
        complexity_data = {
            'Metric': ['Reading Ease Score', 'Grade Level', 'Avg Sentence Length'],
            'Value': [stats['complexity_score'], stats['grade_level'],
                     stats['total_words']/max(stats['total_sentences'], 1)]
        }

        fig_complexity = px.bar(
            x=complexity_data['Metric'],
            y=complexity_data['Value'],
            title="üéì Reading Complexity Analysis",
            color=complexity_data['Value'],
            color_continuous_scale='viridis'
        )
        fig_complexity.update_layout(
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            font=dict(color='white', size=12),
            title=dict(x=0.5, font=dict(size=18))
        )

def create_interface():
    """Create a colorful Gradio interface with interactive quizzes"""

    custom_css = """
    .gradio-container {
        background: linear-gradient(135deg, #fdfbfb, #ebedee);
        font-family: "Poppins", sans-serif;
    }
    .gr-button-primary {
        background: linear-gradient(90deg, #ff9966, #ff5e62);
        border: none !important;
        color: white !important;
        font-weight: bold;
    }
    .gr-button-secondary {
        background: linear-gradient(90deg, #36d1dc, #5b86e5);
        border: none !important;
        color: white !important;
    }
    .gr-chatbot {
        background: #ffffffcc;
        border-radius: 15px !important;
        box-shadow: 0px 4px 20px rgba(0,0,0,0.15);
    }
    """

    with gr.Blocks(css=custom_css, theme=gr.themes.Soft(), title="üåà Granite AI PDF Chatbot") as interface:
        gr.HTML("""
        <div style="text-align:center; padding:20px; background:#ffe0e9; border-radius:15px; margin-bottom:15px;">
            <h1 style="color:#d72638; font-size:36px;">ü§ñ IBM Granite 3.3 2B Chatbot</h1>
            <h3 style="color:#444;">‚ú® Upload PDFs ‚Üí Generate Q&A ‚Üí Take Quizzes ‚Üí Chat ‚ú®</h3>
        </div>
        """)

        with gr.Tab("üìÑ PDF Upload & Analysis"):
            with gr.Row():
                with gr.Column(scale=1):
                    pdf_input = gr.File(
                        label="üìÇ Upload PDF Document",
                        file_types=[".pdf"],
                        file_count="single"
                    )
                    num_questions = gr.Slider(
                        minimum=1, maximum=10, value=5, step=1,
                        label="Number of Q&A pairs üéØ"
                    )
                    num_quiz = gr.Slider(
                        minimum=1, maximum=5, value=3, step=1,
                        label="Number of Quiz Questions üìù"
                    )
                    process_btn = gr.Button("üöÄ Process PDF", variant="primary")
                with gr.Column(scale=1):
                    status_output = gr.Textbox(label="üìä Processing Status", lines=2)

            with gr.Row():
                qa_output = gr.Markdown(label="üìò Generated Q&A Pairs")
                quiz_output = gr.Markdown(label="üéØ Generated Quiz Questions (Static Preview)")

        with gr.Tab("üìù Interactive Quiz"):
            quiz_question = gr.Textbox(label="Current Question", interactive=False)
            options = gr.Radio(choices=["A", "B", "C", "D"], label="Select your answer")
            submit_answer = gr.Button("‚úÖ Submit Answer", variant="primary")
            feedback = gr.Markdown(label="Feedback")

            # Store quiz questions in state
            quiz_state = gr.State([])  # will hold parsed quiz list
            current_index = gr.State(0)

            def load_quiz():
                return bot.quiz_questions, 0, (
                    bot.quiz_questions[0]['question'] if bot.quiz_questions else "No quiz available"
                )

            def check_answer(choice, quiz_list, index):
                if not quiz_list or index >= len(quiz_list):
                    return "‚ö† No quiz loaded."
                q = quiz_list[index]
                correct = q['correct_answer'].strip().upper()
                if choice.strip().upper() == correct:
                    return f"‚úÖ Correct! üéâ\n\n*Explanation:* {q.get('explanation', '')}"
                else:
                    return f"‚ùå Wrong! The correct answer was *{correct}\n\nExplanation:* {q.get('explanation', '')}"

            # Load quiz after processing
            process_btn.click(
                fn=lambda *args: bot.process_pdf_and_generate_content(*args),
                inputs=[pdf_input, num_questions, num_quiz],
                outputs=[status_output, qa_output, quiz_output]
            ).then(
                fn=load_quiz,
                outputs=[quiz_state, current_index, quiz_question]
            )

            submit_answer.click(
                fn=check_answer,
                inputs=[options, quiz_state, current_index],
                outputs=[feedback]
            )

        with gr.Tab("üí¨ Chat with AI"):
            chatbot = gr.Chatbot(height=400, label="üó® Chat with Granite AI")
            msg = gr.Textbox(
                label="üí° Ask a Question",
                placeholder="Type here... (about PDF or anything!)",
                lines=2
            )
            with gr.Row():
                send_btn = gr.Button("üì® Send", variant="primary")
                clear_btn = gr.Button("üßπ Clear Chat", variant="secondary")

        with gr.Tab("‚Ñπ Instructions"):
            gr.Markdown("""
            ## How to Use
            1. Upload a PDF ‚Üí Process it
            2. See generated Q&A and quiz
            3. Try the *Interactive Quiz* tab
            4. Chat with AI about the PDF

            ‚úÖ Correct/Wrong answers are shown with explanations in the Quiz tab.
            üí° Tip: Use text-based PDFs for best results.
            """)

        # Event handlers (chat only)
        def chat_fn(message, history):
            if not message:
                return history, ""
            history = history or []
            history.append([message, None])
            response = bot.chat_response(message, history)
            history[-1][1] = response
            return history, ""

        def clear_chat():
            return [], ""

        send_btn.click(fn=chat_fn, inputs=[msg, chatbot], outputs=[chatbot, msg])
        msg.submit(fn=chat_fn, inputs=[msg, chatbot], outputs=[chatbot, msg])
        clear_btn.click(fn=clear_chat, outputs=[chatbot])

    return interface

# IBM Granite 3.3 2b Chatbot with PDF Analysis for Google Colab
# Step-by-step implementation

# Step 1: Install required packages
!pip install transformers torch accelerate
!pip install PyPDF2 python-docx
!pip install gradio
!pip install sentence-transformers
!pip install langchain langchain-community
!pip install chromadb
!pip install datasets

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import PyPDF2
import io
import gradio as gr
import json
import re
from typing import List, Dict, Any
import random


def create_interface():
    """Create the Gradio interface for the chatbot"""

    custom_css = """
    .gradio-container {
        background: linear-gradient(135deg, #fdfbfb, #ebedee);
        font-family: "Poppins", sans-serif;
    }
    .gr-button-primary {
        background: linear-gradient(90deg, #ff9966, #ff5e62);
        border: none !important;
        color: white !important;
        font-weight: bold;
        border-radius: 12px !important;
        box-shadow: 0px 4px 12px rgba(0,0,0,0.15);
    }
    .gr-button-secondary {
        background: linear-gradient(90deg, #36d1dc, #5b86e5);
        border: none !important;
        color: white !important;
        font-weight: bold;
        border-radius: 12px !important;
        box-shadow: 0px 4px 12px rgba(0,0,0,0.15);
    }
    .gr-chatbot {
        background: #ffffffdd !important;
        border-radius: 15px !important;
        padding: 15px !important;
        box-shadow: 0px 4px 20px rgba(0,0,0,0.15);
    }
    h1, h2, h3 {
        font-weight: bold;
        background: -webkit-linear-gradient(#ff5e62, #ff9966);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    """

    with gr.Blocks(css=custom_css, theme=gr.themes.Soft(), title="üåà IBM Granite 3.0 2b Chatbot") as interface:
        gr.HTML("""
        <div style="text-align:center; padding:20px; background:#ffe0e9; border-radius:15px; margin-bottom:15px;">
            <h1>ü§ñ IBM Granite 3.0 2b Chatbot</h1>
            <h3>‚ú® Upload PDFs ‚Üí Generate Q&A ‚Üí Take Quizzes ‚Üí Chat ‚ú®</h3>
        </div>
        """)

        with gr.Tab("üìÑ PDF Upload & Analysis"):
            with gr.Row():
                with gr.Column():
                    pdf_input = gr.File(
                        label="üìÇ Upload PDF Document",
                        file_types=[".pdf"],
                        file_count="single"
                    )
                    num_questions = gr.Slider(
                        minimum=1, maximum=10, value=5, step=1,
                        label="üéØ Number of Q&A pairs"
                    )
                    num_quiz = gr.Slider(
                        minimum=1, maximum=5, value=3, step=1,
                        label="üìù Number of quiz questions"
                    )
                    process_btn = gr.Button("üöÄ Process PDF", variant="primary")

                with gr.Column():
                    status_output = gr.Textbox(label="üìä Processing Status", lines=2)

            with gr.Row():
                qa_output = gr.Markdown(label="üìò Generated Q&A Pairs")
                quiz_output = gr.Markdown(label="üéØ Generated Quiz Questions")

        with gr.Tab("üí¨ Chat with AI"):
            chatbot = gr.Chatbot(height=400, label="üí¨ Chat with Granite AI")
            msg = gr.Textbox(
                label="üí° Your Message",
                placeholder="Ask questions about the uploaded PDF or chat normally...",
                lines=2
            )

            with gr.Row():
                send_btn = gr.Button("üì® Send", variant="primary")
                clear_btn = gr.Button("üßπ Clear Chat", variant="secondary")

        with gr.Tab("‚Ñπ Instructions"):
            gr.Markdown("""
            ## How to Use
            1. *Upload PDF* ‚Üí Process with the bot
            2. *Explore Q&A* ‚Üí Generated automatically
            3. *Try Quizzes* ‚Üí Test yourself üìù
            4. *Chat with AI* ‚Üí Ask anything about your document

            üé® This interface is styled with gradients, shadows, and modern typography for a colorful experience.
            """)

        # Event handlers stay the same
        def chat_fn(message, history):
            if not message:
                return history, ""
            history = history or []
            history.append([message, None])
            response = bot.chat_response(message, history)
            history[-1][1] = response
            return history, ""

        def clear_chat():
            return [], ""

        process_btn.click(
            fn=bot.process_pdf_and_generate_content,
            inputs=[pdf_input, num_questions, num_quiz],
            outputs=[status_output, qa_output, quiz_output]
        )
        send_btn.click(fn=chat_fn, inputs=[msg, chatbot], outputs=[chatbot, msg])
        msg.submit(fn=chat_fn, inputs=[msg, chatbot], outputs=[chatbot, msg])
        clear_btn.click(fn=clear_chat, outputs=[chatbot])

    return interface

# Step 2: Initialize the IBM Granite model
class GraniteBot:
    def init(self):
        self.model_name = "ibm-granite/granite-3.0-2b-instruct"
        print("Loading IBM Granite 3.0 2b model...")

        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )

        # Create text generation pipeline
        self.generator = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )

        # Storage for PDF content and generated Q&As
        self.pdf_content = ""
        self.qa_pairs = []
        self.quiz_questions = []

        print("Model loaded successfully!")

    # Step 3: PDF Processing Functions
    def extract_text_from_pdf(self, pdf_file):
        """Extract text content from uploaded PDF file"""
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file))
            text = ""

            for page_num in range(len(pdf_reader.pages)):
                page = pdf_reader.pages[page_num]
                text += page.extract_text() + "\n"

            return text.strip()
        except Exception as e:
            return f"Error extracting PDF: {str(e)}"

    def chunk_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        """Split text into manageable chunks for processing"""
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i + chunk_size])
            chunks.append(chunk)

        return chunks

    # Step 4: Question Generation Functions
    def generate_questions_from_text(self, text: str, num_questions: int = 5) -> List[Dict]:
        """Generate questions and answers from the given text"""
        chunks = self.chunk_text(text, 800)
        all_qa_pairs = []

        for chunk in chunks[:3]:  # Process first 3 chunks to avoid token limits
            prompt = f"""Based on the following text, generate {num_questions} diverse questions and their answers.
            Format your response as:
            Q: [Question]
            A: [Answer]

            Text: {chunk}

            Generate questions that test comprehension, analysis, and key facts:"""

            try:
                response = self.generator(
                    prompt,
                    max_new_tokens=500,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

                generated_text = response[0]['generated_text'][len(prompt):]
                qa_pairs = self.parse_qa_response(generated_text)
                all_qa_pairs.extend(qa_pairs)

            except Exception as e:
                print(f"Error generating questions: {e}")

        return all_qa_pairs[:num_questions]

    def parse_qa_response(self, response: str) -> List[Dict]:
        """Parse the generated response into Q&A pairs"""
        qa_pairs = []
        lines = response.split('\n')
        current_q = ""
        current_a = ""

        for line in lines:
            line = line.strip()
            if line.startswith('Q:'):
                if current_q and current_a:
                    qa_pairs.append({"question": current_q, "answer": current_a})
                current_q = line[2:].strip()
                current_a = ""
            elif line.startswith('A:'):
                current_a = line[2:].strip()
            elif current_a and line:
                current_a += " " + line

        # Add the last Q&A pair
        if current_q and current_a:
            qa_pairs.append({"question": current_q, "answer": current_a})

        return qa_pairs

    # Step 5: Quiz Generation Functions
    def generate_quiz(self, num_questions: int = 5) -> List[Dict]:
        """Generate multiple choice quiz questions from PDF content"""
        if not self.pdf_content:
            return [{"error": "No PDF content loaded. Please upload a PDF first."}]

        chunks = self.chunk_text(self.pdf_content, 600)
        quiz_questions = []

        for i, chunk in enumerate(chunks[:num_questions]):
            prompt = f"""Based on this text, create 1 multiple choice question with 4 options (A, B, C, D) and indicate the correct answer.

            Text: {chunk}

            Format:
            Question: [Your question here]
            A) [Option A]
            B) [Option B]
            C) [Option C]
            D) [Option D]
            Correct Answer: [A/B/C/D]
            Explanation: [Brief explanation]"""

            try:
                response = self.generator(
                    prompt,
                    max_new_tokens=300,
                    temperature=0.6,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

                generated_text = response[0]['generated_text'][len(prompt):]
                quiz_question = self.parse_quiz_response(generated_text)
                if quiz_question:
                    quiz_questions.append(quiz_question)

            except Exception as e:
                print(f"Error generating quiz question {i+1}: {e}")

        return quiz_questions

    def parse_quiz_response(self, response: str) -> Dict:
        """Parse quiz response into structured format"""
        lines = [line.strip() for line in response.split('\n') if line.strip()]
        quiz_data = {}

        for line in lines:
            if line.startswith('Question:'):
                quiz_data['question'] = line[9:].strip()
            elif line.startswith('A)'):
                quiz_data['option_a'] = line[2:].strip()
            elif line.startswith('B)'):
                quiz_data['option_b'] = line[2:].strip()
            elif line.startswith('C)'):
                quiz_data['option_c'] = line[2:].strip()
            elif line.startswith('D)'):
                quiz_data['option_d'] = line[2:].strip()
            elif line.startswith('Correct Answer:'):
                quiz_data['correct_answer'] = line[15:].strip()
            elif line.startswith('Explanation:'):
                quiz_data['explanation'] = line[12:].strip()

        return quiz_data if len(quiz_data) >= 6 else None

    # Step 6: Chat Functions
    def chat_response(self, message: str, history: List = None) -> str:
        """Generate chat response using the model"""
        if not message.strip():
            return "Please enter a message."

        # Context-aware response if PDF is loaded
        context = ""
        if self.pdf_content:
            context = f"Context from uploaded document: {self.pdf_content[:500]}...\n\n"

        prompt = f"""{context}User: {message}
Assistant: """

        try:
            response = self.generator(
                prompt,
                max_new_tokens=200,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

            generated_text = response[0]['generated_text'][len(prompt):].strip()
            return generated_text

        except Exception as e:
            return f"Error generating response: {str(e)}"

    # Step 7: Main Processing Function
    def process_pdf_and_generate_content(self, pdf_file, num_questions=5, num_quiz=3):
        """Main function to process PDF and generate all content"""
        if pdf_file is None:
            return "Please upload a PDF file.", "", ""

        # Extract text from PDF
        self.pdf_content = self.extract_text_from_pdf(pdf_file)

        if self.pdf_content.startswith("Error"):
            return self.pdf_content, "", ""

        # Generate Q&A pairs
        print("Generating questions and answers...")
        self.qa_pairs = self.generate_questions_from_text(self.pdf_content, num_questions)

        # Generate quiz questions
        print("Generating quiz questions...")
        self.quiz_questions = self.generate_quiz(num_quiz)

        # Format results
        qa_text = self.format_qa_pairs()
        quiz_text = self.format_quiz_questions()

        return f"PDF processed successfully! Extracted {len(self.pdf_content)} characters.", qa_text, quiz_text

    def format_qa_pairs(self) -> str:
        """Format Q&A pairs for display"""
        if not self.qa_pairs:
            return "No questions generated."

        formatted = "üìù Generated Questions & Answers:\n\n"
        for i, qa in enumerate(self.qa_pairs, 1):
            formatted += f"Q{i}: {qa.get('question', 'No question')}\n"
            formatted += f"A{i}: {qa.get('answer', 'No answer')}\n\n"

        return formatted

    def format_quiz_questions(self) -> str:
        """Format quiz questions for display"""
        if not self.quiz_questions:
            return "No quiz questions generated."

        formatted = "üéØ Quiz Questions:\n\n"
        for i, quiz in enumerate(self.quiz_questions, 1):
            if 'error' in quiz:
                continue

            formatted += f"Question {i}: {quiz.get('question', 'No question')}\n"
            formatted += f"A) {quiz.get('option_a', 'No option')}\n"
            formatted += f"B) {quiz.get('option_b', 'No option')}\n"
            formatted += f"C) {quiz.get('option_c', 'No option')}\n"
            formatted += f"D) {quiz.get('option_d', 'No option')}\n"
            formatted += f"Correct Answer: {quiz.get('correct_answer', 'Unknown')}\n"
            formatted += f"Explanation: {quiz.get('explanation', 'No explanation')}\n\n"

        return formatted

# Step 8: Initialize the bot
print("Initializing Granite Bot...")
bot = GraniteBot()

# Step 9: Create Gradio Interface
def create_interface():
    """Create the Gradio interface for the chatbot"""

    with gr.Blocks(title="IBM Granite 3.0 2b Chatbot with PDF Analysis", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ü§ñ IBM Granite 3.0 2b Chatbot with PDF Analysis")
        gr.Markdown("Upload a PDF document and interact with an AI chatbot that can analyze the content, generate questions, and create quizzes!")

        with gr.Tab("üìÑ PDF Upload & Analysis"):
            with gr.Row():
                with gr.Column():
                    pdf_input = gr.File(
                        label="Upload PDF Document",
                        file_types=[".pdf"],
                        file_count="single"
                    )
                    num_questions = gr.Slider(
                        minimum=1, maximum=10, value=5, step=1,
                        label="Number of Q&A pairs to generate"
                    )
                    num_quiz = gr.Slider(
                        minimum=1, maximum=5, value=3, step=1,
                        label="Number of quiz questions to generate"
                    )
                    process_btn = gr.Button("üîÑ Process PDF", variant="primary")

                with gr.Column():
                    status_output = gr.Textbox(label="Processing Status", lines=2)

            with gr.Row():
                with gr.Column():
                    qa_output = gr.Markdown(label="Generated Q&A Pairs")

                with gr.Column():
                    quiz_output = gr.Markdown(label="Generated Quiz Questions")

        with gr.Tab("üí¨ Chat with AI"):
            chatbot = gr.Chatbot(height=400, label="Chat with Granite AI")
            msg = gr.Textbox(
                label="Your Message",
                placeholder="Ask questions about the uploaded PDF or chat normally...",
                lines=2
            )

            with gr.Row():
                send_btn = gr.Button("Send", variant="primary")
                clear_btn = gr.Button("Clear Chat")

        with gr.Tab("‚Ñπ Instructions"):
            gr.Markdown("""
            ## How to Use This Chatbot:

            ### 1. PDF Upload & Analysis
            - Go to the "PDF Upload & Analysis" tab
            - Upload a PDF document using the file picker
            - Adjust the number of questions and quiz items you want
            - Click "Process PDF" to analyze the document

            ### 2. Features Available:
            - Question & Answer Generation: Automatically creates Q&A pairs from your PDF
            - Quiz Creation: Generates multiple-choice questions with explanations
            - Interactive Chat: Chat with the AI about the PDF content or any topic

            ### 3. Chat with AI
            - Use the "Chat with AI" tab to have conversations
            - The AI will use the uploaded PDF as context for relevant questions
            - Ask about specific topics from your document or general questions

            ### 4. Tips for Best Results:
            - Upload clear, text-based PDFs (not scanned images)
            - PDFs with structured content work better
            - Use specific questions for more detailed answers
            - The AI remembers the PDF content throughout your session

            ### Model Information:
            - Model: IBM Granite 3.0 2b Instruct
            - Capabilities: Text generation, Q&A, summarization, analysis
            - Context: Uses uploaded PDF content to provide relevant responses
            """)

        # Event handlers
        def chat_fn(message, history):
            if not message:
                return history, ""

            # Add user message to history
            history = history or []
            history.append([message, None])

            # Get bot response
            response = bot.chat_response(message, history)
            history[-1][1] = response

            return history, ""

        def clear_chat():
            return [], ""

        # Connect event handlers
        process_btn.click(
            fn=bot.process_pdf_and_generate_content,
            inputs=[pdf_input, num_questions, num_quiz],
            outputs=[status_output, qa_output, quiz_output]
        )

        send_btn.click(
            fn=chat_fn,
            inputs=[msg, chatbot],
            outputs=[chatbot, msg]
        )

        msg.submit(
            fn=chat_fn,
            inputs=[msg, chatbot],
            outputs=[chatbot, msg]
        )

        clear_btn.click(
            fn=clear_chat,
            outputs=[chatbot]
        )

    return interface

# Step 10: Launch the application
if __name__ == "__main__":
    print("Creating interface...")
    interface = create_interface()

    print("Launching Gradio interface...")
    interface.launch(
        share=True,  # Creates a public link for sharing
        debug=True,
        server_name="0.0.0.0",
        server_port=7860
    )

# Additional utility functions for advanced features
class AdvancedFeatures:
    """Additional features for the chatbot"""

    @staticmethod
    def export_qa_to_json(qa_pairs: List[Dict], filename: str = "qa_pairs.json"):
        """Export generated Q&A pairs to JSON file"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(qa_pairs, f, indent=2, ensure_ascii=False)
        return f"Q&A pairs exported to {filename}"

    @staticmethod
    def create_study_guide(pdf_content: str, qa_pairs: List[Dict]) -> str:
        """Create a formatted study guide"""
        study_guide = f"""
# Study Guide

## Document Summary
Content length: {len(pdf_content)} characters
Generated: {len(qa_pairs)} question-answer pairs

## Key Questions & Answers
"""
        for i, qa in enumerate(qa_pairs, 1):
            study_guide += f"\n### Question {i}\n"
            study_guide += f"Q: {qa.get('question', 'No question')}\n"
            study_guide += f"A: {qa.get('answer', 'No answer')}\n"

        return study_guide

print("üéâ IBM Granite 3.0 2b Chatbot setup complete!")
print("üìã Features available:")
print("   ‚úÖ PDF text extraction and analysis")
print("   ‚úÖ Automatic Q&A generation")
print("   ‚úÖ Quiz creation with multiple choice questions")
print("   ‚úÖ Interactive chat with document context")
print("   ‚úÖ Gradio web interface")
print("\nüöÄ Run the code to launch your chatbot!")

# IBM Granite 3.3 2b Chatbot with PDF Analysis for Google Colab
# Step-by-step implementation

# Step 1: Install required packages
!pip install transformers torch accelerate
!pip install PyPDF2 python-docx
!pip install gradio
!pip install sentence-transformers
!pip install langchain langchain-community
!pip install chromadb
!pip install datasets

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import PyPDF2
import io
import gradio as gr
import json
import re
from typing import List, Dict, Any
import random


def create_interface():
    """Create the Gradio interface for the chatbot"""

    custom_css = """
    .gradio-container {
        background: linear-gradient(135deg, #fdfbfb, #ebedee);
        font-family: "Poppins", sans-serif;
    }
    .gr-button-primary {
        background: linear-gradient(90deg, #ff9966, #ff5e62);
        border: none !important;
        color: white !important;
        font-weight: bold;
        border-radius: 12px !important;
        box-shadow: 0px 4px 12px rgba(0,0,0,0.15);
    }
    .gr-button-secondary {
        background: linear-gradient(90deg, #36d1dc, #5b86e5);
        border: none !important;
        color: white !important;
        font-weight: bold;
        border-radius: 12px !important;
        box-shadow: 0px 4px 12px rgba(0,0,0,0.15);
    }
    .gr-chatbot {
        background: #ffffffdd !important;
        border-radius: 15px !important;
        padding: 15px !important;
        box-shadow: 0px 4px 20px rgba(0,0,0,0.15);
    }
    h1, h2, h3 {
        font-weight: bold;
        background: -webkit-linear-gradient(#ff5e62, #ff9966);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    """

    with gr.Blocks(css=custom_css, theme=gr.themes.Soft(), title="üåà IBM Granite 3.0 2b Chatbot") as interface:
        gr.HTML("""
        <div style="text-align:center; padding:20px; background:#ffe0e9; border-radius:15px; margin-bottom:15px;">
            <h1>ü§ñ IBM Granite 3.0 2b Chatbot</h1>
            <h3>‚ú® Upload PDFs ‚Üí Generate Q&A ‚Üí Take Quizzes ‚Üí Chat ‚ú®</h3>
        </div>
        """)

        with gr.Tab("üìÑ PDF Upload & Analysis"):
            with gr.Row():
                with gr.Column():
                    pdf_input = gr.File(
                        label="üìÇ Upload PDF Document",
                        file_types=[".pdf"],
                        file_count="single"
                    )
                    num_questions = gr.Slider(
                        minimum=1, maximum=10, value=5, step=1,
                        label="üéØ Number of Q&A pairs"
                    )
                    num_quiz = gr.Slider(
                        minimum=1, maximum=5, value=3, step=1,
                        label="üìù Number of quiz questions"
                    )
                    process_btn = gr.Button("üöÄ Process PDF", variant="primary")

                with gr.Column():
                    status_output = gr.Textbox(label="üìä Processing Status", lines=2)

            with gr.Row():
                qa_output = gr.Markdown(label="üìò Generated Q&A Pairs")
                quiz_output = gr.Markdown(label="üéØ Generated Quiz Questions")

        with gr.Tab("üí¨ Chat with AI"):
            chatbot = gr.Chatbot(height=400, label="üí¨ Chat with Granite AI")
            msg = gr.Textbox(
                label="üí° Your Message",
                placeholder="Ask questions about the uploaded PDF or chat normally...",
                lines=2
            )

            with gr.Row():
                send_btn = gr.Button("üì® Send", variant="primary")
                clear_btn = gr.Button("üßπ Clear Chat", variant="secondary")

        with gr.Tab("‚Ñπ Instructions"):
            gr.Markdown("""
            ## How to Use
            1. *Upload PDF* ‚Üí Process with the bot
            2. *Explore Q&A* ‚Üí Generated automatically
            3. *Try Quizzes* ‚Üí Test yourself üìù
            4. *Chat with AI* ‚Üí Ask anything about your document

            üé® This interface is styled with gradients, shadows, and modern typography for a colorful experience.
            """)

        # Event handlers stay the same
        def chat_fn(message, history):
            if not message:
                return history, ""
            history = history or []
            history.append([message, None])
            response = bot.chat_response(message, history)
            history[-1][1] = response
            return history, ""

        def clear_chat():
            return [], ""

        process_btn.click(
            fn=bot.process_pdf_and_generate_content,
            inputs=[pdf_input, num_questions, num_quiz],
            outputs=[status_output, qa_output, quiz_output]
        )
        send_btn.click(fn=chat_fn, inputs=[msg, chatbot], outputs=[chatbot, msg])
        msg.submit(fn=chat_fn, inputs=[msg, chatbot], outputs=[chatbot, msg])
        clear_btn.click(fn=clear_chat, outputs=[chatbot])

    return interface

# Step 2: Initialize the IBM Granite model
class GraniteBot:
    def init(self):
        self.model_name = "ibm-granite/granite-3.0-2b-instruct"
        print("Loading IBM Granite 3.0 2b model...")

        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )

        # Create text generation pipeline
        self.generator = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )

        # Storage for PDF content and generated Q&As
        self.pdf_content = ""
        self.qa_pairs = []
        self.quiz_questions = []

        print("Model loaded successfully!")

    # Step 3: PDF Processing Functions
    def extract_text_from_pdf(self, pdf_file):
        """Extract text content from uploaded PDF file"""
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file))
            text = ""

            for page_num in range(len(pdf_reader.pages)):
                page = pdf_reader.pages[page_num]
                text += page.extract_text() + "\n"

            return text.strip()
        except Exception as e:
            return f"Error extracting PDF: {str(e)}"

    def chunk_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        """Split text into manageable chunks for processing"""
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i + chunk_size])
            chunks.append(chunk)

        return chunks

    # Step 4: Question Generation Functions
    def generate_questions_from_text(self, text: str, num_questions: int = 5) -> List[Dict]:
        """Generate questions and answers from the given text"""
        chunks = self.chunk_text(text, 800)
        all_qa_pairs = []

        for chunk in chunks[:3]:  # Process first 3 chunks to avoid token limits
            prompt = f"""Based on the following text, generate {num_questions} diverse questions and their answers.
            Format your response as:
            Q: [Question]
            A: [Answer]

            Text: {chunk}

            Generate questions that test comprehension, analysis, and key facts:"""

            try:
                response = self.generator(
                    prompt,
                    max_new_tokens=500,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

                generated_text = response[0]['generated_text'][len(prompt):]
                qa_pairs = self.parse_qa_response(generated_text)
                all_qa_pairs.extend(qa_pairs)

            except Exception as e:
                print(f"Error generating questions: {e}")

        return all_qa_pairs[:num_questions]

    def parse_qa_response(self, response: str) -> List[Dict]:
        """Parse the generated response into Q&A pairs"""
        qa_pairs = []
        lines = response.split('\n')
        current_q = ""
        current_a = ""

        for line in lines:
            line = line.strip()
            if line.startswith('Q:'):
                if current_q and current_a:
                    qa_pairs.append({"question": current_q, "answer": current_a})
                current_q = line[2:].strip()
                current_a = ""
            elif line.startswith('A:'):
                current_a = line[2:].strip()
            elif current_a and line:
                current_a += " " + line

        # Add the last Q&A pair
        if current_q and current_a:
            qa_pairs.append({"question": current_q, "answer": current_a})

        return qa_pairs

    # Step 5: Quiz Generation Functions
    def generate_quiz(self, num_questions: int = 5) -> List[Dict]:
        """Generate multiple choice quiz questions from PDF content"""
        if not self.pdf_content:
            return [{"error": "No PDF content loaded. Please upload a PDF first."}]

        chunks = self.chunk_text(self.pdf_content, 600)
        quiz_questions = []

        for i, chunk in enumerate(chunks[:num_questions]):
            prompt = f"""Based on this text, create 1 multiple choice question with 4 options (A, B, C, D) and indicate the correct answer.

            Text: {chunk}

            Format:
            Question: [Your question here]
            A) [Option A]
            B) [Option B]
            C) [Option C]
            D) [Option D]
            Correct Answer: [A/B/C/D]
            Explanation: [Brief explanation]"""

            try:
                response = self.generator(
                    prompt,
                    max_new_tokens=300,
                    temperature=0.6,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

                generated_text = response[0]['generated_text'][len(prompt):]
                quiz_question = self.parse_quiz_response(generated_text)
                if quiz_question:
                    quiz_questions.append(quiz_question)

            except Exception as e:
                print(f"Error generating quiz question {i+1}: {e}")

        return quiz_questions

    def parse_quiz_response(self, response: str) -> Dict:
        """Parse quiz response into structured format"""
        lines = [line.strip() for line in response.split('\n') if line.strip()]
        quiz_data = {}

        for line in lines:
            if line.startswith('Question:'):
                quiz_data['question'] = line[9:].strip()
            elif line.startswith('A)'):
                quiz_data['option_a'] = line[2:].strip()
            elif line.startswith('B)'):
                quiz_data['option_b'] = line[2:].strip()
            elif line.startswith('C)'):
                quiz_data['option_c'] = line[2:].strip()
            elif line.startswith('D)'):
                quiz_data['option_d'] = line[2:].strip()
            elif line.startswith('Correct Answer:'):
                quiz_data['correct_answer'] = line[15:].strip()
            elif line.startswith('Explanation:'):
                quiz_data['explanation'] = line[12:].strip()

        return quiz_data if len(quiz_data) >= 6 else None

    # Step 6: Chat Functions
    def chat_response(self, message: str, history: List = None) -> str:
        """Generate chat response using the model"""
        if not message.strip():
            return "Please enter a message."

        # Context-aware response if PDF is loaded
        context = ""
        if self.pdf_content:
            context = f"Context from uploaded document: {self.pdf_content[:500]}...\n\n"

        prompt = f"""{context}User: {message}
Assistant: """

        try:
            response = self.generator(
                prompt,
                max_new_tokens=200,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

            generated_text = response[0]['generated_text'][len(prompt):].strip()
            return generated_text

        except Exception as e:
            return f"Error generating response: {str(e)}"

    # Step 7: Main Processing Function
    def process_pdf_and_generate_content(self, pdf_file, num_questions=5, num_quiz=3):
        """Main function to process PDF and generate all content"""
        if pdf_file is None:
            return "Please upload a PDF file.", "", ""

        # Extract text from PDF
        self.pdf_content = self.extract_text_from_pdf(pdf_file)

        if self.pdf_content.startswith("Error"):
            return self.pdf_content, "", ""

        # Generate Q&A pairs
        print("Generating questions and answers...")
        self.qa_pairs = self.generate_questions_from_text(self.pdf_content, num_questions)

        # Generate quiz questions
        print("Generating quiz questions...")
        self.quiz_questions = self.generate_quiz(num_quiz)

        # Format results
        qa_text = self.format_qa_pairs()
        quiz_text = self.format_quiz_questions()

        return f"PDF processed successfully! Extracted {len(self.pdf_content)} characters.", qa_text, quiz_text

    def format_qa_pairs(self) -> str:
        """Format Q&A pairs for display"""
        if not self.qa_pairs:
            return "No questions generated."

        formatted = "üìù Generated Questions & Answers:\n\n"
        for i, qa in enumerate(self.qa_pairs, 1):
            formatted += f"Q{i}: {qa.get('question', 'No question')}\n"
            formatted += f"A{i}: {qa.get('answer', 'No answer')}\n\n"

        return formatted

    def format_quiz_questions(self) -> str:
        """Format quiz questions for display"""
        if not self.quiz_questions:
            return "No quiz questions generated."

        formatted = "üéØ Quiz Questions:\n\n"
        for i, quiz in enumerate(self.quiz_questions, 1):
            if 'error' in quiz:
                continue

            formatted += f"Question {i}: {quiz.get('question', 'No question')}\n"
            formatted += f"A) {quiz.get('option_a', 'No option')}\n"
            formatted += f"B) {quiz.get('option_b', 'No option')}\n"
            formatted += f"C) {quiz.get('option_c', 'No option')}\n"
            formatted += f"D) {quiz.get('option_d', 'No option')}\n"
            formatted += f"Correct Answer: {quiz.get('correct_answer', 'Unknown')}\n"
            formatted += f"Explanation: {quiz.get('explanation', 'No explanation')}\n\n"

        return formatted

# Step 8: Initialize the bot
print("Initializing Granite Bot...")
bot = GraniteBot()

# Step 9: Create Gradio Interface
def create_interface():
    """Create the Gradio interface for the chatbot"""

    with gr.Blocks(title="IBM Granite 3.0 2b Chatbot with PDF Analysis", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ü§ñ IBM Granite 3.0 2b Chatbot with PDF Analysis")
        gr.Markdown("Upload a PDF document and interact with an AI chatbot that can analyze the content, generate questions, and create quizzes!")

        with gr.Tab("üìÑ PDF Upload & Analysis"):
            with gr.Row():
                with gr.Column():
                    pdf_input = gr.File(
                        label="Upload PDF Document",
                        file_types=[".pdf"],
                        file_count="single"
                    )
                    num_questions = gr.Slider(
                        minimum=1, maximum=10, value=5, step=1,
                        label="Number of Q&A pairs to generate"
                    )
                    num_quiz = gr.Slider(
                        minimum=1, maximum=5, value=3, step=1,
                        label="Number of quiz questions to generate"
                    )
                    process_btn = gr.Button("üîÑ Process PDF", variant="primary")

                with gr.Column():
                    status_output = gr.Textbox(label="Processing Status", lines=2)

            with gr.Row():
                with gr.Column():
                    qa_output = gr.Markdown(label="Generated Q&A Pairs")

                with gr.Column():
                    quiz_output = gr.Markdown(label="Generated Quiz Questions")

        with gr.Tab("üí¨ Chat with AI"):
            chatbot = gr.Chatbot(height=400, label="Chat with Granite AI")
            msg = gr.Textbox(
                label="Your Message",
                placeholder="Ask questions about the uploaded PDF or chat normally...",
                lines=2
            )

            with gr.Row():
                send_btn = gr.Button("Send", variant="primary")
                clear_btn = gr.Button("Clear Chat")

        with gr.Tab("‚Ñπ Instructions"):
            gr.Markdown("""
            ## How to Use This Chatbot:

            ### 1. PDF Upload & Analysis
            - Go to the "PDF Upload & Analysis" tab
            - Upload a PDF document using the file picker
            - Adjust the number of questions and quiz items you want
            - Click "Process PDF" to analyze the document

            ### 2. Features Available:
            - Question & Answer Generation: Automatically creates Q&A pairs from your PDF
            - Quiz Creation: Generates multiple-choice questions with explanations
            - Interactive Chat: Chat with the AI about the PDF content or any topic

            ### 3. Chat with AI
            - Use the "Chat with AI" tab to have conversations
            - The AI will use the uploaded PDF as context for relevant questions
            - Ask about specific topics from your document or general questions

            ### 4. Tips for Best Results:
            - Upload clear, text-based PDFs (not scanned images)
            - PDFs with structured content work better
            - Use specific questions for more detailed answers
            - The AI remembers the PDF content throughout your session

            ### Model Information:
            - Model: IBM Granite 3.0 2b Instruct
            - Capabilities: Text generation, Q&A, summarization, analysis
            - Context: Uses uploaded PDF content to provide relevant responses
            """)

        # Event handlers
        def chat_fn(message, history):
            if not message:
                return history, ""

            # Add user message to history
            history = history or []
            history.append([message, None])

            # Get bot response
            response = bot.chat_response(message, history)
            history[-1][1] = response

            return history, ""

        def clear_chat():
            return [], ""

        # Connect event handlers
        process_btn.click(
            fn=bot.process_pdf_and_generate_content,
            inputs=[pdf_input, num_questions, num_quiz],
            outputs=[status_output, qa_output, quiz_output]
        )

        send_btn.click(
            fn=chat_fn,
            inputs=[msg, chatbot],
            outputs=[chatbot, msg]
        )

        msg.submit(
            fn=chat_fn,
            inputs=[msg, chatbot],
            outputs=[chatbot, msg]
        )

        clear_btn.click(
            fn=clear_chat,
            outputs=[chatbot]
        )

    return interface

# Step 10: Launch the application
if __name__ == "__main__":
    print("Creating interface...")
    interface = create_interface()

    print("Launching Gradio interface...")
    interface.launch(
        share=True,  # Creates a public link for sharing
        debug=True,
        server_name="0.0.0.0",
        server_port=7860
    )

# Additional utility functions for advanced features
class AdvancedFeatures:
    """Additional features for the chatbot"""

    @staticmethod
    def export_qa_to_json(qa_pairs: List[Dict], filename: str = "qa_pairs.json"):
        """Export generated Q&A pairs to JSON file"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(qa_pairs, f, indent=2, ensure_ascii=False)
        return f"Q&A pairs exported to {filename}"

    @staticmethod
    def create_study_guide(pdf_content: str, qa_pairs: List[Dict]) -> str:
        """Create a formatted study guide"""
        study_guide = f"""
# Study Guide

## Document Summary
Content length: {len(pdf_content)} characters
Generated: {len(qa_pairs)} question-answer pairs

## Key Questions & Answers
"""
        for i, qa in enumerate(qa_pairs, 1):
            study_guide += f"\n### Question {i}\n"
            study_guide += f"Q: {qa.get('question', 'No question')}\n"
            study_guide += f"A: {qa.get('answer', 'No answer')}\n"

        return study_guide

print("üéâ IBM Granite 3.0 2b Chatbot setup complete!")
print("üìã Features available:")
print("   ‚úÖ PDF text extraction and analysis")
print("   ‚úÖ Automatic Q&A generation")
print("   ‚úÖ Quiz creation with multiple choice questions")
print("   ‚úÖ Interactive chat with document context")
print("   ‚úÖ Gradio web interface")
print("\nüöÄ Run the code to launch your chatbot!")

# IBM Granite 3.0 2b Chatbot with PDF Analysis for Google Colab
# Fixed and Complete Implementation

# Step 1: Install required packages
!pip install transformers torch accelerate
!pip install PyPDF2 python-docx
!pip install gradio
!pip install sentence-transformers
!pip install langchain langchain-community
!pip install chromadb
!pip install datasets

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import PyPDF2
import io
import gradio as gr
import json
import re
from typing import List, Dict, Any
import random

# Step 2: Initialize the IBM Granite model
class GraniteBot:
    def _init(self):  # Fixed: was _init, now _init_
        self.model_name = "ibm-granite/granite-3.0-2b-instruct"
        print("Loading IBM Granite 3.0 2b model...")

        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

        # Add pad token if it doesn't exist
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )

        # Create text generation pipeline
        self.generator = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )

        # Storage for PDF content and generated Q&As
        self.pdf_content = ""
        self.qa_pairs = []
        self.quiz_questions = []

        print("Model loaded successfully!")

    # Step 3: PDF Processing Functions
    def extract_text_from_pdf(self, pdf_file):
        """Extract text content from uploaded PDF file"""
        try:
            # Handle different input types
            if hasattr(pdf_file, 'read'):
                pdf_bytes = pdf_file.read()
            elif isinstance(pdf_file, str):
                with open(pdf_file, 'rb') as f:
                    pdf_bytes = f.read()
            else:
                pdf_bytes = pdf_file

            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))
            text = ""

            for page_num in range(len(pdf_reader.pages)):
                page = pdf_reader.pages[page_num]
                text += page.extract_text() + "\n"

            return text.strip()
        except Exception as e:
            return f"Error extracting PDF: {str(e)}"

    def chunk_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        """Split text into manageable chunks for processing"""
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i + chunk_size])
            chunks.append(chunk)

        return chunks

    # Step 4: Question Generation Functions
    def generate_questions_from_text(self, text: str, num_questions: int = 5) -> List[Dict]:
        """Generate questions and answers from the given text"""
        if not text or len(text.strip()) < 50:
            return [{"question": "No content available", "answer": "Please upload a valid PDF with text content."}]

        chunks = self.chunk_text(text, 800)
        all_qa_pairs = []

        for i, chunk in enumerate(chunks[:min(3, len(chunks))]):  # Process first 3 chunks to avoid token limits
            print(f"Processing chunk {i+1}/{min(3, len(chunks))}")

            # Create a more structured prompt
            prompt = f"""<|system|>
You are a helpful assistant that generates educational questions and answers based on provided text.

<|user|>
Based on the following text, generate 2-3 questions with detailed answers. Focus on key concepts and important information.

Text: {chunk[:800]}

Please format your response exactly like this:
Q: [Write a clear, specific question]
A: [Write a comprehensive answer]

Q: [Write another clear, specific question]
A: [Write a comprehensive answer]

<|assistant|>"""

            try:
                response = self.generator(
                    prompt,
                    max_new_tokens=400,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )

                generated_text = response[0]['generated_text'][len(prompt):]
                qa_pairs = self.parse_qa_response(generated_text)
                all_qa_pairs.extend(qa_pairs)

                if len(all_qa_pairs) >= num_questions:
                    break

            except Exception as e:
                print(f"Error generating questions for chunk {i+1}: {e}")
                continue

        # Return requested number of questions or all generated ones
        return all_qa_pairs[:num_questions] if all_qa_pairs else [{"question": "Could not generate questions", "answer": "Please try with a different PDF or check the content."}]

    def parse_qa_response(self, response: str) -> List[Dict]:
        """Parse the generated response into Q&A pairs"""
        qa_pairs = []
        lines = response.split('\n')
        current_q = ""
        current_a = ""

        for line in lines:
            line = line.strip()
            if line.startswith('Q:'):
                if current_q and current_a:
                    qa_pairs.append({"question": current_q.strip(), "answer": current_a.strip()})
                current_q = line[2:].strip()
                current_a = ""
            elif line.startswith('A:'):
                current_a = line[2:].strip()
            elif current_a and line and not line.startswith('Q:'):
                current_a += " " + line

        # Add the last Q&A pair
        if current_q and current_a:
            qa_pairs.append({"question": current_q.strip(), "answer": current_a.strip()})

        return qa_pairs

    # Step 5: Quiz Generation Functions
    def generate_quiz(self, num_questions: int = 3) -> List[Dict]:
        """Generate multiple choice quiz questions from PDF content"""
        if not self.pdf_content:
            return [{"error": "No PDF content loaded. Please upload a PDF first."}]

        chunks = self.chunk_text(self.pdf_content, 600)
        quiz_questions = []

        for i, chunk in enumerate(chunks[:num_questions]):
            print(f"Generating quiz question {i+1}/{num_questions}")

            prompt = f"""<|system|>
You are a quiz generator that creates multiple choice questions based on provided text.

<|user|>
Based on this text, create 1 multiple choice question with 4 options (A, B, C, D) and indicate the correct answer.

Text: {chunk[:600]}

Format your response exactly like this:
Question: [Your question here]
A) [Option A]
B) [Option B]
C) [Option C]
D) [Option D]
Correct Answer: A
Explanation: [Brief explanation of why this is correct]

<|assistant|>"""

            try:
                response = self.generator(
                    prompt,
                    max_new_tokens=300,
                    temperature=0.6,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )

                generated_text = response[0]['generated_text'][len(prompt):]
                quiz_question = self.parse_quiz_response(generated_text)
                if quiz_question:
                    quiz_questions.append(quiz_question)

            except Exception as e:
                print(f"Error generating quiz question {i+1}: {e}")
                continue

        return quiz_questions if quiz_questions else [{"error": "Could not generate quiz questions"}]

    def parse_quiz_response(self, response: str) -> Dict:
        """Parse quiz response into structured format"""
        lines = [line.strip() for line in response.split('\n') if line.strip()]
        quiz_data = {}

        for line in lines:
            if line.startswith('Question:'):
                quiz_data['question'] = line[9:].strip()
            elif line.startswith('A)'):
                quiz_data['option_a'] = line[2:].strip()
            elif line.startswith('B)'):
                quiz_data['option_b'] = line[2:].strip()
            elif line.startswith('C)'):
                quiz_data['option_c'] = line[2:].strip()
            elif line.startswith('D)'):
                quiz_data['option_d'] = line[2:].strip()
            elif line.startswith('Correct Answer:'):
                quiz_data['correct_answer'] = line[15:].strip()
            elif line.startswith('Explanation:'):
                quiz_data['explanation'] = line[12:].strip()

        # Validate that we have all required fields
        required_fields = ['question', 'option_a', 'option_b', 'option_c', 'option_d', 'correct_answer']
        if all(field in quiz_data for field in required_fields):
            return quiz_data
        else:
            return None

    # Step 6: Chat Functions
    def chat_response(self, message: str, history: List = None) -> str:
        """Generate chat response using the model"""
        if not message.strip():
            return "Please enter a message."

        # Build context from PDF and chat history
        context_parts = []

        if self.pdf_content:
            context_parts.append(f"Context from uploaded document:\n{self.pdf_content[:1000]}...")

        # Build conversation history
        if history:
            context_parts.append("Previous conversation:")
            for user_msg, bot_msg in history[-3:]:  # Last 3 exchanges
                if user_msg and bot_msg:
                    context_parts.append(f"User: {user_msg}")
                    context_parts.append(f"Assistant: {bot_msg}")

        context = "\n\n".join(context_parts) if context_parts else ""

        prompt = f"""<|system|>
You are a helpful AI assistant. If context from a document is provided, use it to answer questions accurately. Be concise and informative.

<|user|>
{context}

Current question: {message}

<|assistant|>"""

        try:
            response = self.generator(
                prompt,
                max_new_tokens=200,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1
            )

            generated_text = response[0]['generated_text'][len(prompt):].strip()

            # Clean up the response
            if generated_text:
                # Remove any system tokens that might appear
                generated_text = re.sub(r'<\|.*?\|>', '', generated_text).strip()
                return generated_text[:500]  # Limit response length
            else:
                return "I'm sorry, I couldn't generate a proper response. Please try rephrasing your question."

        except Exception as e:
            return f"Error generating response: {str(e)}"

    # Step 7: Main Processing Function
    def process_pdf_and_generate_content(self, pdf_file, num_questions=5, num_quiz=3):
        """Main function to process PDF and generate all content"""
        if pdf_file is None:
            return "Please upload a PDF file.", "No Q&A generated.", "No quiz generated."

        try:
            # Extract text from PDF
            print("Extracting text from PDF...")
            self.pdf_content = self.extract_text_from_pdf(pdf_file)

            if self.pdf_content.startswith("Error"):
                return self.pdf_content, "Could not extract text from PDF.", "Could not generate quiz."

            if len(self.pdf_content.strip()) < 50:
                return "PDF appears to be empty or contains no readable text.", "No content to process.", "No content to process."

            # Generate Q&A pairs
            print("Generating questions and answers...")
            self.qa_pairs = self.generate_questions_from_text(self.pdf_content, num_questions)

            # Generate quiz questions
            print("Generating quiz questions...")
            self.quiz_questions = self.generate_quiz(num_quiz)

            # Format results
            qa_text = self.format_qa_pairs()
            quiz_text = self.format_quiz_questions()

            status_msg = f"‚úÖ PDF processed successfully!\nüìÑ Extracted {len(self.pdf_content)} characters\n‚ùì Generated {len(self.qa_pairs)} Q&A pairs\nüéØ Generated {len([q for q in self.quiz_questions if 'error' not in q])} quiz questions"

            return status_msg, qa_text, quiz_text

        except Exception as e:
            error_msg = f"Error processing PDF: {str(e)}"
            return error_msg, "Could not generate Q&A.", "Could not generate quiz."

    def format_qa_pairs(self) -> str:
        """Format Q&A pairs for display"""
        if not self.qa_pairs:
            return "No questions generated."

        formatted = "# üìù Generated Questions & Answers\n\n"
        for i, qa in enumerate(self.qa_pairs, 1):
            formatted += f"## Question {i}\n"
            formatted += f"*Q:* {qa.get('question', 'No question')}\n\n"
            formatted += f"*A:* {qa.get('answer', 'No answer')}\n\n"
            formatted += "---\n\n"

        return formatted

    def format_quiz_questions(self) -> str:
        """Format quiz questions for display"""
        if not self.quiz_questions:
            return "No quiz questions generated."

        formatted = "# üéØ Quiz Questions\n\n"
        quiz_count = 1

        for quiz in self.quiz_questions:
            if 'error' in quiz:
                continue

            formatted += f"## Question {quiz_count}\n"
            formatted += f"{quiz.get('question', 'No question')}\n\n"
            formatted += f"A) {quiz.get('option_a', 'No option')}\n\n"
            formatted += f"B) {quiz.get('option_b', 'No option')}\n\n"
            formatted += f"C) {quiz.get('option_c', 'No option')}\n\n"
            formatted += f"D) {quiz.get('option_d', 'No option')}\n\n"
            formatted += f"*Correct Answer:* {quiz.get('correct_answer', 'Unknown')}\n\n"
            if quiz.get('explanation'):
                formatted += f"*Explanation:* {quiz.get('explanation')}\n\n"
            formatted += "---\n\n"
            quiz_count += 1

        return formatted

# Step 8: Create Gradio Interface
def create_interface():
    """Create the Gradio interface for the chatbot"""

    # Initialize the bot
    print("Initializing Granite Bot...")
    bot = GraniteBot()

    with gr.Blocks(title="IBM Granite 3.0 2b Chatbot with PDF Analysis", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ü§ñ IBM Granite 3.0 2b Chatbot with PDF Analysis")
        gr.Markdown("Upload a PDF document and interact with an AI chatbot that can analyze the content, generate questions, and create quizzes!")

        with gr.Tab("üìÑ PDF Upload & Analysis"):
            with gr.Row():
                with gr.Column():
                    pdf_input = gr.File(
                        label="Upload PDF Document",
                        file_types=[".pdf"],
                        file_count="single"
                    )
                    num_questions = gr.Slider(
                        minimum=1, maximum=10, value=5, step=1,
                        label="Number of Q&A pairs to generate"
                    )
                    num_quiz = gr.Slider(
                        minimum=1, maximum=5, value=3, step=1,
                        label="Number of quiz questions to generate"
                    )
                    process_btn = gr.Button("üîÑ Process PDF", variant="primary")

                with gr.Column():
                    status_output = gr.Textbox(label="Processing Status", lines=4)

            with gr.Row():
                with gr.Column():
                    qa_output = gr.Markdown(label="Generated Q&A Pairs")

                with gr.Column():
                    quiz_output = gr.Markdown(label="Generated Quiz Questions")

        with gr.Tab("üí¨ Chat with AI"):
            chatbot = gr.Chatbot(height=400, label="Chat with Granite AI")
            msg = gr.Textbox(
                label="Your Message",
                placeholder="Ask questions about the uploaded PDF or chat normally...",
                lines=2
            )

            with gr.Row():
                send_btn = gr.Button("Send", variant="primary")
                clear_btn = gr.Button("Clear Chat")

        with gr.Tab("‚Ñπ Instructions"):
            gr.Markdown("""
            ## How to Use This Chatbot:

            ### 1. PDF Upload & Analysis
            - Go to the "PDF Upload & Analysis" tab
            - Upload a PDF document using the file picker
            - Adjust the number of questions and quiz items you want
            - Click "Process PDF" to analyze the document

            ### 2. Features Available:
            - *Question & Answer Generation*: Automatically creates Q&A pairs from your PDF
            - *Quiz Creation*: Generates multiple-choice questions with explanations
            - *Interactive Chat*: Chat with the AI about the PDF content or any topic

            ### 3. Chat with AI
            - Use the "Chat with AI" tab to have conversations
            - The AI will use the uploaded PDF as context for relevant questions
            - Ask about specific topics from your document or general questions

            ### 4. Tips for Best Results:
            - Upload clear, text-based PDFs (not scanned images)
            - PDFs with structured content work better
            - Use specific questions for more detailed answers
            - The AI remembers the PDF content throughout your session

            ### Model Information:
            - *Model*: IBM Granite 3.0 2b Instruct
            - *Capabilities*: Text generation, Q&A, summarization, analysis
            - *Context*: Uses uploaded PDF content to provide relevant responses
            """)

        # Event handlers
        def chat_fn(message, history):
            if not message:
                return history, ""

            # Add user message to history
            history = history or []
            history.append([message, None])

            # Get bot response
            response = bot.chat_response(message, history[:-1])  # Don't include current incomplete exchange
            history[-1][1] = response

            return history, ""

        def clear_chat():
            return [], ""

        # Connect event handlers
        process_btn.click(
            fn=bot.process_pdf_and_generate_content,
            inputs=[pdf_input, num_questions, num_quiz],
            outputs=[status_output, qa_output, quiz_output]
        )

        send_btn.click(
            fn=chat_fn,

# üåü SmartStudy AI - PDF Tutor, Quiz Builder & Knowledge Helper
!pip install pymupdf transformers accelerate gradio

import fitz  # PyMuPDF
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import gradio as gr
import os
from datetime import datetime

# -------------------------
# Enable GPU optimizations
# -------------------------
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True

# -------------------------
# Load Model & Tokenizer
# -------------------------
MODEL_ID = "ibm-granite/granite-3.2-2b-instruct"
tokenizer_ssai = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
model_ssai = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="auto",
    torch_dtype="auto"
)

ai_generator = pipeline(
    "text-generation",
    model=model_ssai,
    tokenizer=tokenizer_ssai
)

# -------------------------
# PDF Extraction Settings
# -------------------------
MAX_PAGES = 3
MAX_TEXT = 2000
MAX_OUTPUT_TOKENS = 400

# -------------------------
# PDF Text Extraction
# -------------------------
def extract_pdf_text(pdf_file):
    if pdf_file is None:
        return ""
    try:
        doc = fitz.open(pdf_file.name)
        text_content = ""
        for i in range(min(len(doc), MAX_PAGES)):
            text_content += doc.load_page(i).get_text("text") + "\n"
        doc.close()
        return text_content.strip()[:MAX_TEXT]
    except Exception as e:
        return f"‚ö† PDF Read Error: {str(e)}"

def is_academic(text):
    keywords = ["math", "physics", "chemistry", "biology", "history", "geography",
                "computer science", "ai", "machine learning", "education", "economics",
                "philosophy", "english", "engineering", "statistics", "data"]
    return any(word in text.lower() for word in keywords)

# -------------------------
# AI Functions
# -------------------------
def explain_pdf(pdf_file):
    content = extract_pdf_text(pdf_file)
    if not content or not is_academic(content):
        return "‚ö† Please upload a valid educational PDF.", None

    prompt = f"Explain this educational content clearly and thoroughly:\n{content}\nExplanation:"
    result = ai_generator(prompt, max_new_tokens=MAX_OUTPUT_TOKENS, temperature=0.7, top_p=0.9)[0]["generated_text"]
    explanation = result[len(prompt):].strip()

    # Save explanation
    save_file = f"smartstudy_explanation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    save_path = os.path.join(os.getcwd(), save_file)
    with open(save_path, "w", encoding="utf-8") as f:
        f.write(explanation)

    return explanation, save_path

def generate_quiz(pdf_file):
    content = extract_pdf_text(pdf_file)
    if not content or not is_academic(content):
        return "‚ö† Please upload a valid educational PDF."

    prompt = (
        f"Create a 5-question multiple choice quiz from this educational content. "
        f"Include four options (A, B, C, D) with correct answers indicated.\n\n"
        f"{content}\nQuiz:"
    )
    result = ai_generator(prompt, max_new_tokens=MAX_OUTPUT_TOKENS, temperature=0.7, top_p=0.9)[0]["generated_text"]
    return result[len(prompt):].strip()

def answer_pdf(pdf_file, question):
    content = extract_pdf_text(pdf_file)
    if not content or not is_academic(content):
        return "‚ö† Please upload a valid educational PDF."
    if not question.strip():
        return "‚ö† Please enter a question."

    prompt = f"Based on this educational content:\n{content}\nAnswer the following question clearly:\n{question}\nAnswer:"
    result = ai_generator(prompt, max_new_tokens=MAX_OUTPUT_TOKENS, temperature=0.7, top_p=0.9)[0]["generated_text"]
    return result[len(prompt):].strip()

# -------------------------
# Gradio UI
# -------------------------
with gr.Blocks(css="""
body {background: linear-gradient(135deg, #caffbf, #fdffb6); font-family: 'Montserrat', sans-serif;}
.gradio-container {background: rgba(255,255,255,0.95); border-radius: 25px; padding: 40px; max-width: 950px; margin:auto; box-shadow: 0 12px 35px rgba(0,0,0,0.25);}
h1 {color:#ff7f50; text-align:center; font-size:50px; font-weight:700; text-shadow:2px 2px #ffe4e1;}
h3 {color:#ffb347; text-align:center; font-size:26px; margin-bottom:35px;}
.gr-button {background: linear-gradient(90deg, #6a11cb, #2575fc) !important; color:white !important; font-weight:700; border-radius:14px; padding:12px 22px !important; font-size:18px;}
.gr-button:hover {background: linear-gradient(90deg, #2575fc, #6a11cb) !important; transform:translateY(-2px);}
.gr-textbox textarea {background:#f0f8ff !important; border-radius:14px; padding:15px; font-size:16px; box-shadow: inset 0 0 8px rgba(0,0,0,0.05);}
.gr-file input[type=file] {background:#f0f8ff !important; border-radius:14px; padding:8px;}
""") as smartstudy_app:

    gr.Markdown("<h1>üåü SmartStudy AI</h1>")
    gr.Markdown("<h3>PDF Tutor, Quiz Builder & Knowledge Helper</h3>")

    with gr.Tab("üìñ PDF Tutor"):
        upload_file = gr.File(label="Upload PDF")
        display_text = gr.Textbox(label="Explanation", lines=10)
        download_file = gr.File(label="Download Explanation")
        btn_explain = gr.Button("Explain & Save")
        btn_explain.click(explain_pdf, inputs=upload_file, outputs=[display_text, download_file])

    with gr.Tab("üìù Quiz Builder"):
        upload_quiz_file = gr.File(label="Upload PDF")
        display_quiz = gr.Textbox(label="Quiz Content", lines=15)
        btn_generate_quiz = gr.Button("Generate Quiz")
        btn_generate_quiz.click(generate_quiz, inputs=upload_quiz_file, outputs=display_quiz)

    with gr.Tab("üí° Ask Questions"):
        upload_qa_file = gr.File(label="Upload PDF")
        question_box = gr.Textbox(label="Enter Question")
        answer_box = gr.Textbox(label="Answer", lines=10)
        btn_answer = gr.Button("Get Answer")
        btn_answer.click(answer_pdf, inputs=[upload_qa_file, question_box], outputs=answer_box)

smartstudy_app.launch()

# üåü SmartStudy AI - PDF Tutor, Quiz Builder & Knowledge Helper
!pip install pymupdf transformers accelerate gradio

import fitz  # PyMuPDF
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import gradio as gr
import os
from datetime import datetime

# -------------------------
# Enable GPU optimizations
# -------------------------
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True

# -------------------------
# Load Model & Tokenizer
# -------------------------
MODEL_ID = "ibm-granite/granite-3.2-2b-instruct"
tokenizer_ssai = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
model_ssai = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="auto",
    torch_dtype="auto"
)

ai_generator = pipeline(
    "text-generation",
    model=model_ssai,
    tokenizer=tokenizer_ssai
)

# -------------------------
# PDF Extraction Settings
# -------------------------
MAX_PAGES = 3
MAX_TEXT = 2000
MAX_OUTPUT_TOKENS = 400

# -------------------------
# PDF Text Extraction
# -------------------------
def extract_pdf_text(pdf_file):
    if pdf_file is None:
        return ""
    try:
        doc = fitz.open(pdf_file.name)
        text_content = ""
        for i in range(min(len(doc), MAX_PAGES)):
            text_content += doc.load_page(i).get_text("text") + "\n"
        doc.close()
        return text_content.strip()[:MAX_TEXT]
    except Exception as e:
        return f"‚ö† PDF Read Error: {str(e)}"

def is_academic(text):
    keywords = ["math", "physics", "chemistry", "biology", "history", "geography",
                "computer science", "ai", "machine learning", "education", "economics",
                "philosophy", "english", "engineering", "statistics", "data"]
    return any(word in text.lower() for word in keywords)

def count_pdf_characters(pdf_file):
    """Return total characters and total words in the PDF"""
    content = extract_pdf_text(pdf_file)
    total_chars = len(content)
    total_words = len(content.split())
    return f"Total Characters: {total_chars}\nTotal Words: {total_words}"

# -------------------------
# AI Functions
# -------------------------
def explain_pdf(pdf_file):
    content = extract_pdf_text(pdf_file)
    if not content or not is_academic(content):
        return "‚ö† Please upload a valid educational PDF.", None, ""

    prompt = f"Explain this educational content clearly and thoroughly:\n{content}\nExplanation:"
    result = ai_generator(prompt, max_new_tokens=MAX_OUTPUT_TOKENS, temperature=0.7, top_p=0.9)[0]["generated_text"]
    explanation = result[len(prompt):].strip()

    # Save explanation
    save_file = f"smartstudy_explanation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    save_path = os.path.join(os.getcwd(), save_file)
    with open(save_path, "w", encoding="utf-8") as f:
        f.write(explanation)

    # Count characters & words
    stats = count_pdf_characters(pdf_file)

    return explanation, save_path, stats

def generate_quiz(pdf_file):
    content = extract_pdf_text(pdf_file)
    if not content or not is_academic(content):
        return "‚ö† Please upload a valid educational PDF."

    prompt = (
        f"Create a 5-question multiple choice quiz from this educational content. "
        f"Include four options (A, B, C, D) with correct answers indicated.\n\n"
        f"{content}\nQuiz:"
    )
    result = ai_generator(prompt, max_new_tokens=MAX_OUTPUT_TOKENS, temperature=0.7, top_p=0.9)[0]["generated_text"]
    return result[len(prompt):].strip()

def answer_pdf(pdf_file, question):
    content = extract_pdf_text(pdf_file)
    if not content or not is_academic(content):
        return "‚ö† Please upload a valid educational PDF."
    if not question.strip():
        return "‚ö† Please enter a question."

    prompt = f"Based on this educational content:\n{content}\nAnswer the following question clearly:\n{question}\nAnswer:"
    result = ai_generator(prompt, max_new_tokens=MAX_OUTPUT_TOKENS, temperature=0.7, top_p=0.9)[0]["generated_text"]
    return result[len(prompt):].strip()

# -------------------------
# Gradio UI
# -------------------------
with gr.Blocks(css="""
body {background: linear-gradient(135deg, #caffbf, #fdffb6); font-family: 'Montserrat', sans-serif;}
.gradio-container {background: rgba(255,255,255,0.95); border-radius: 25px; padding: 40px; max-width: 950px; margin:auto; box-shadow: 0 12px 35px rgba(0,0,0,0.25);}
h1 {color:#ff7f50; text-align:center; font-size:50px; font-weight:700; text-shadow:2px 2px #ffe4e1;}
h3 {color:#ffb347; text-align:center; font-size:26px; margin-bottom:35px;}
.gr-button {background: linear-gradient(90deg, #6a11cb, #2575fc) !important; color:white !important; font-weight:700; border-radius:14px; padding:12px 22px !important; font-size:18px;}
.gr-button:hover {background: linear-gradient(90deg, #2575fc, #6a11cb) !important; transform:translateY(-2px);}
.gr-textbox textarea {background:#f0f8ff !important; border-radius:14px; padding:15px; font-size:16px; box-shadow: inset 0 0 8px rgba(0,0,0,0.05);}
.gr-file input[type=file] {background:#f0f8ff !important; border-radius:14px; padding:8px;}
""") as smartstudy_app:

    gr.Markdown("<h1>üåü SmartStudy AI</h1>")
    gr.Markdown("<h3>PDF Tutor, Quiz Builder & Knowledge Helper</h3>")

    with gr.Tab("üìñ PDF Tutor"):
        upload_file = gr.File(label="Upload PDF")
        display_text = gr.Textbox(label="Explanation", lines=10)
        download_file = gr.File(label="Download Explanation")
        char_stats = gr.Textbox(label="PDF Stats", lines=2)
        btn_explain = gr.Button("Explain & Save")
        btn_explain.click(explain_pdf, inputs=upload_file, outputs=[display_text, download_file, char_stats])

    with gr.Tab("üìù Quiz Builder"):
        upload_quiz_file = gr.File(label="Upload PDF")
        display_quiz = gr.Textbox(label="Quiz Content", lines=15)
        btn_generate_quiz = gr.Button("Generate Quiz")
        btn_generate_quiz.click(generate_quiz, inputs=upload_quiz_file, outputs=display_quiz)

    with gr.Tab("üí° Ask Questions"):
        upload_qa_file = gr.File(label="Upload PDF")
        question_box = gr.Textbox(label="Enter Question")
        answer_box = gr.Textbox(label="Answer", lines=10)
        btn_answer = gr.Button("Get Answer")
        btn_answer.click(answer_pdf, inputs=[upload_qa_file, question_box], outputs=answer_box)

smartstudy_app.launch()

# IBM Granite 3.3 2b Chatbot with PDF Analysis (Colorful Front-End)

!pip install transformers torch accelerate
!pip install PyPDF2 python-docx
!pip install gradio
!pip install sentence-transformers
!pip install langchain langchain-community
!pip install chromadb
!pip install datasets

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import PyPDF2
import io
import gradio as gr
from typing import List, Dict

# ----------------- Step 2: Granite Bot -----------------
class GraniteBot:
    def init(self):
        self.model_name = "ibm-granite/granite-3.0-2b-instruct"
        print("Loading IBM Granite 3.0 2b model...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        self.generator = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        self.pdf_content = ""
        self.qa_pairs = []
        self.quiz_questions = []
        print("Model loaded successfully!")

    # ---------- PDF Functions ----------
    def extract_text_from_pdf(self, pdf_file):
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file))
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text.strip()
        except Exception as e:
            return f"Error extracting PDF: {str(e)}"

    def chunk_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        words = text.split()
        return [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

    # ---------- Q&A ----------
    def generate_questions_from_text(self, text: str, num_questions: int = 5) -> List[Dict]:
        chunks = self.chunk_text(text, 800)
        all_qa_pairs = []
        for chunk in chunks[:3]:
            prompt = f"""Based on this text, generate {num_questions} Q&A pairs.
Text: {chunk}
Format:
Q: [Question]
A: [Answer]"""
            try:
                response = self.generator(prompt, max_new_tokens=500, temperature=0.7, do_sample=True,
                                          pad_token_id=self.tokenizer.eos_token_id)
                generated_text = response[0]['generated_text'][len(prompt):]
                all_qa_pairs.extend(self.parse_qa_response(generated_text))
            except:
                pass
        return all_qa_pairs[:num_questions]

    def parse_qa_response(self, response: str) -> List[Dict]:
        qa_pairs = []
        lines = response.split("\n")
        current_q, current_a = "", ""
        for line in lines:
            line = line.strip()
            if line.startswith("Q:"):
                if current_q and current_a:
                    qa_pairs.append({"question": current_q, "answer": current_a})
                current_q = line[2:].strip()
                current_a = ""
            elif line.startswith("A:"):
                current_a = line[2:].strip()
            elif current_a and line:
                current_a += " " + line
        if current_q and current_a:
            qa_pairs.append({"question": current_q, "answer": current_a})
        return qa_pairs

    # ---------- Quiz ----------
    def generate_quiz(self, num_questions: int = 5) -> List[Dict]:
        if not self.pdf_content:
            return [{"error": "No PDF content loaded"}]
        chunks = self.chunk_text(self.pdf_content, 600)
        quiz_questions = []
        for chunk in chunks[:num_questions]:
            prompt = f"""Create 1 multiple choice question from this text:
Text: {chunk}
Format:
Question: [Your question]
A) [Option A]
B) [Option B]
C) [Option C]
D) [Option D]
Correct Answer: [A/B/C/D]
Explanation: [Brief explanation]"""
            try:
                response = self.generator(prompt, max_new_tokens=300, temperature=0.6, do_sample=True,
                                          pad_token_id=self.tokenizer.eos_token_id)
                generated_text = response[0]['generated_text'][len(prompt):]
                q = self.parse_quiz_response(generated_text)
                if q:
                    quiz_questions.append(q)
            except:
                pass
        return quiz_questions

    def parse_quiz_response(self, response: str) -> Dict:
        lines = [l.strip() for l in response.split("\n") if l.strip()]
        quiz = {}
        for line in lines:
            if line.startswith("Question:"):
                quiz['question'] = line[9:].strip()
            elif line.startswith("A)"):
                quiz['option_a'] = line[2:].strip()
            elif line.startswith("B)"):
                quiz['option_b'] = line[2:].strip()
            elif line.startswith("C)"):
                quiz['option_c'] = line[2:].strip()
            elif line.startswith("D)"):
                quiz['option_d'] = line[2:].strip()
            elif line.startswith("Correct Answer:"):
                quiz['correct_answer'] = line[15:].strip()
            elif line.startswith("Explanation:"):
                quiz['explanation'] = line[12:].strip()
        return quiz if len(quiz) >= 6 else None

    # ---------- Chat ----------
    def chat_response(self, message: str, history: List = None) -> str:
        if not message.strip():
            return "Please enter a message."
        context = f"Context: {self.pdf_content[:500]}...\n\n" if self.pdf_content else ""
        prompt = f"{context}User: {message}\nAssistant: "
        try:
            response = self.generator(prompt, max_new_tokens=200, temperature=0.7, do_sample=True,
                                      pad_token_id=self.tokenizer.eos_token_id)
            return response[0]['generated_text'][len(prompt):].strip()
        except:
            return "Error generating response."

    # ---------- Process PDF ----------
    def process_pdf_and_generate_content(self, pdf_file, num_questions=5, num_quiz=3):
        if pdf_file is None:
            return "Please upload a PDF file.", "", ""
        self.pdf_content = self.extract_text_from_pdf(pdf_file)
        if self.pdf_content.startswith("Error"):
            return self.pdf_content, "", ""
        self.qa_pairs = self.generate_questions_from_text(self.pdf_content, num_questions)
        self.quiz_questions = self.generate_quiz(num_quiz)
        return "‚úÖ PDF Processed!", self.format_qa_pairs(), self.format_quiz_questions()

    def format_qa_pairs(self) -> str:
        return "\n".join([f"Q{i+1}: {q['question']}\nA{i+1}: {q['answer']}" for i, q in enumerate(self.qa_pairs)])

    def format_quiz_questions(self) -> str:
        result = ""
        for i, q in enumerate(self.quiz_questions):
            if 'error' in q: continue
            result += f"Q{i+1}: {q['question']}\nA) {q['option_a']}\nB) {q['option_b']}\nC) {q['option_c']}\nD) {q['option_d']}\nAnswer: {q['correct_answer']}\nExplanation: {q['explanation']}\n\n"
        return result

# ----------------- Initialize Bot -----------------
bot = GraniteBot()

# ----------------- Custom CSS -----------------
custom_css = """
body { background-color: #fdf6e3; font-family: 'Arial', sans-serif; }
h1, h2 { color: #6a0dad; }
.gr-button { background: linear-gradient(90deg,#ff7f50,#ff4500); color: white; font-weight: bold; border-radius: 12px; }
.gr-file-upload, .gr-textbox, .gr-slider { border: 2px solid #6a0dad; border-radius: 8px; padding: 5px; }
.gr-markdown { background-color: #fff0f5; padding: 10px; border-radius: 8px; border: 1px solid #dda0dd; }
.gr-chatbot { border: 2px solid #ff69b4; border-radius: 12px; }
"""

# ----------------- Gradio Interface -----------------
def create_interface():
    with gr.Blocks(title="IBM Granite PDF Chatbot", theme=gr.themes.Soft(), css=custom_css) as interface:
        gr.Markdown("# ü§ñ IBM Granite PDF Chatbot")
        gr.Markdown("Upload a PDF, generate Q&A, quizzes, and chat with AI!")

        with gr.Tab("üìÑ PDF Analysis"):
            with gr.Row():
                with gr.Column():
                    pdf_input = gr.File(label="Upload PDF", file_types=[".pdf"])
                    num_questions = gr.Slider(1,10,5,label="Q&A Pairs")
                    num_quiz = gr.Slider(1,5,3,label="Quiz Questions")
                    process_btn = gr.Button("Process PDF")
                with gr.Column():
                    status_output = gr.Textbox(label="Status")
            with gr.Row():
                qa_output = gr.Markdown(label="Generated Q&A")
                quiz_output = gr.Markdown(label="Generated Quiz")

        with gr.Tab("üí¨ Chat AI"):
            chatbot = gr.Chatbot(height=400)
            msg = gr.Textbox(label="Message", placeholder="Ask something...")
            with gr.Row():
                send_btn = gr.Button("Send")
                clear_btn = gr.Button("Clear")

        # Event Handlers
        def chat_fn(message, history):
            history = history or []
            history.append([message, None])
            response = bot.chat_response(message, history)
            history[-1][1] = response
            return history, ""

        def clear_chat(): return [], ""

        process_btn.click(bot.process_pdf_and_generate_content, inputs=[pdf_input,num_questions,num_quiz], outputs=[status_output,qa_output,quiz_output])
        send_btn.click(chat_fn, inputs=[msg,chatbot], outputs=[chatbot,msg])
        msg.submit(chat_fn, inputs=[msg,chatbot], outputs=[chatbot,msg])
        clear_btn.click(clear_chat, outputs=[chatbot])

    return interface

# ----------------- Launch -----------------
interface = create_interface()
interface.launch(share=True, debug=True)

# IBM Granite 3.3 2b Chatbot with PDF Analysis (Colorful Front-End)

!pip install transformers torch accelerate
!pip install PyPDF2 python-docx
!pip install gradio
!pip install sentence-transformers
!pip install langchain langchain-community
!pip install chromadb
!pip install datasets

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import PyPDF2
import io
import gradio as gr
from typing import List, Dict

# ----------------- Step 2: Granite Bot -----------------
class GraniteBot:
    def init(self):
        self.model_name = "ibm-granite/granite-3.0-2b-instruct"
        print("Loading IBM Granite 3.0 2b model...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        self.generator = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        self.pdf_content = ""
        self.qa_pairs = []
        self.quiz_questions = []
        print("Model loaded successfully!")

    # ---------- PDF Functions ----------
    def extract_text_from_pdf(self, pdf_file):
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file))
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text.strip()
        except Exception as e:
            return f"Error extracting PDF: {str(e)}"

    def chunk_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        words = text.split()
        return [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

    # ---------- Q&A ----------
    def generate_questions_from_text(self, text: str, num_questions: int = 5) -> List[Dict]:
        chunks = self.chunk_text(text, 800)
        all_qa_pairs = []
        for chunk in chunks[:3]:
            prompt = f"""Based on this text, generate {num_questions} Q&A pairs.
Text: {chunk}
Format:
Q: [Question]
A: [Answer]"""
            try:
                response = self.generator(prompt, max_new_tokens=500, temperature=0.7, do_sample=True,
                                          pad_token_id=self.tokenizer.eos_token_id)
                generated_text = response[0]['generated_text'][len(prompt):]
                all_qa_pairs.extend(self.parse_qa_response(generated_text))
            except:
                pass
        return all_qa_pairs[:num_questions]

    def parse_qa_response(self, response: str) -> List[Dict]:
        qa_pairs = []
        lines = response.split("\n")
        current_q, current_a = "", ""
        for line in lines:
            line = line.strip()
            if line.startswith("Q:"):
                if current_q and current_a:
                    qa_pairs.append({"question": current_q, "answer": current_a})
                current_q = line[2:].strip()
                current_a = ""
            elif line.startswith("A:"):
                current_a = line[2:].strip()
            elif current_a and line:
                current_a += " " + line
        if current_q and current_a:
            qa_pairs.append({"question": current_q, "answer": current_a})
        return qa_pairs

    # ---------- Quiz ----------
    def generate_quiz(self, num_questions: int = 5) -> List[Dict]:
        if not self.pdf_content:
            return [{"error": "No PDF content loaded"}]
        chunks = self.chunk_text(self.pdf_content, 600)
        quiz_questions = []
        for chunk in chunks[:num_questions]:
            prompt = f"""Create 1 multiple choice question from this text:
Text: {chunk}
Format:
Question: [Your question]
A) [Option A]
B) [Option B]
C) [Option C]
D) [Option D]
Correct Answer: [A/B/C/D]
Explanation: [Brief explanation]"""
            try:
                response = self.generator(prompt, max_new_tokens=300, temperature=0.6, do_sample=True,
                                          pad_token_id=self.tokenizer.eos_token_id)
                generated_text = response[0]['generated_text'][len(prompt):]
                q = self.parse_quiz_response(generated_text)
                if q:
                    quiz_questions.append(q)
            except:
                pass
        return quiz_questions

    def parse_quiz_response(self, response: str) -> Dict:
        lines = [l.strip() for l in response.split("\n") if l.strip()]
        quiz = {}
        for line in lines:
            if line.startswith("Question:"):
                quiz['question'] = line[9:].strip()
            elif line.startswith("A)"):
                quiz['option_a'] = line[2:].strip()
            elif line.startswith("B)"):
                quiz['option_b'] = line[2:].strip()
            elif line.startswith("C)"):
                quiz['option_c'] = line[2:].strip()
            elif line.startswith("D)"):
                quiz['option_d'] = line[2:].strip()
            elif line.startswith("Correct Answer:"):
                quiz['correct_answer'] = line[15:].strip()
            elif line.startswith("Explanation:"):
                quiz['explanation'] = line[12:].strip()
        return quiz if len(quiz) >= 6 else None

    # ---------- Chat ----------
    def chat_response(self, message: str, history: List = None) -> str:
        if not message.strip():
            return "Please enter a message."
        context = f"Context: {self.pdf_content[:500]}...\n\n" if self.pdf_content else ""
        prompt = f"{context}User: {message}\nAssistant: "
        try:
            response = self.generator(prompt, max_new_tokens=200, temperature=0.7, do_sample=True,
                                      pad_token_id=self.tokenizer.eos_token_id)
            return response[0]['generated_text'][len(prompt):].strip()
        except:
            return "Error generating response."

    # ---------- Process PDF ----------
    def process_pdf_and_generate_content(self, pdf_file, num_questions=5, num_quiz=3):
        if pdf_file is None:
            return "Please upload a PDF file.", "", ""
        self.pdf_content = self.extract_text_from_pdf(pdf_file)
        if self.pdf_content.startswith("Error"):
            return self.pdf_content, "", ""
        self.qa_pairs = self.generate_questions_from_text(self.pdf_content, num_questions)
        self.quiz_questions = self.generate_quiz(num_quiz)
        return "‚úÖ PDF Processed!", self.format_qa_pairs(), self.format_quiz_questions()

    def format_qa_pairs(self) -> str:
        return "\n".join([f"Q{i+1}: {q['question']}\nA{i+1}: {q['answer']}" for i, q in enumerate(self.qa_pairs)])

    def format_quiz_questions(self) -> str:
        result = ""
        for i, q in enumerate(self.quiz_questions):
            if 'error' in q: continue
            result += f"Q{i+1}: {q['question']}\nA) {q['option_a']}\nB) {q['option_b']}\nC) {q['option_c']}\nD) {q['option_d']}\nAnswer: {q['correct_answer']}\nExplanation: {q['explanation']}\n\n"
        return result

# ----------------- Initialize Bot -----------------
bot = GraniteBot()

# ----------------- Custom CSS -----------------
custom_css = """
body { background-color: #fdf6e3; font-family: 'Arial', sans-serif; }
h1, h2 { color: #6a0dad; }
.gr-button { background: linear-gradient(90deg,#ff7f50,#ff4500); color: white; font-weight: bold; border-radius: 12px; }
.gr-file-upload, .gr-textbox, .gr-slider { border: 2px solid #6a0dad; border-radius: 8px; padding: 5px; }
.gr-markdown { background-color: #fff0f5; padding: 10px; border-radius: 8px; border: 1px solid #dda0dd; }
.gr-chatbot { border: 2px solid #ff69b4; border-radius: 12px; }
"""

# ----------------- Gradio Interface -----------------
def create_interface():
    with gr.Blocks(title="IBM Granite PDF Chatbot", theme=gr.themes.Soft(), css=custom_css) as interface:
        gr.Markdown("# ü§ñ IBM Granite PDF Chatbot")
        gr.Markdown("Upload a PDF, generate Q&A, quizzes, and chat with AI!")

        with gr.Tab("üìÑ PDF Analysis"):
            with gr.Row():
                with gr.Column():
                    pdf_input = gr.File(label="Upload PDF", file_types=[".pdf"])
                    num_questions = gr.Slider(1,10,5,label="Q&A Pairs")
                    num_quiz = gr.Slider(1,5,3,label="Quiz Questions")
                    process_btn = gr.Button("Process PDF")
                with gr.Column():
                    status_output = gr.Textbox(label="Status")
            with gr.Row():
                qa_output = gr.Markdown(label="Generated Q&A")
                quiz_output = gr.Markdown(label="Generated Quiz")

        with gr.Tab("üí¨ Chat AI"):
            chatbot = gr.Chatbot(height=400)
            msg = gr.Textbox(label="Message", placeholder="Ask something...")
            with gr.Row():
                send_btn = gr.Button("Send")
                clear_btn = gr.Button("Clear")

        # Event Handlers
        def chat_fn(message, history):
            history = history or []
            history.append([message, None])
            response = bot.chat_response(message, history)
            history[-1][1] = response
            return history, ""

        def clear_chat(): return [], ""

        process_btn.click(bot.process_pdf_and_generate_content, inputs=[pdf_input,num_questions,num_quiz], outputs=[status_output,qa_output,quiz_output])
        send_btn.click(chat_fn, inputs=[msg,chatbot], outputs=[chatbot,msg])
        msg.submit(chat_fn, inputs=[msg,chatbot], outputs=[chatbot,msg])
        clear_btn.click(clear_chat, outputs=[chatbot])

    return interface

# ----------------- Launch -----------------
interface = create_interface()
interface.launch(share=True, debug=True)

from pyngrok import ngrok
ngrok.set_auth_token("32woirZIvGNY6RZccsxY0OhCE30_3cUrnNNQ5MnLpGWYNJJd3")  # paste your token here

# ==============================================================================
# RUN STREAMLIT WITH NGROK
# ==============================================================================
import subprocess
import threading
import time
from pyngrok import ngrok

# Kill previous tunnels
ngrok.kill()

def run_app():
    subprocess.run(["streamlit", "run", "app.py", "--server.port=8501"])

# Run Streamlit in background
thread = threading.Thread(target=run_app)
thread.start()
time.sleep(5)

# Open ngrok tunnel
public_url = ngrok.connect(8501).public_url
print("‚úÖ Streamlit app running at:", public_url)